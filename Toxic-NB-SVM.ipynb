{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3b04218-0413-4e6c-8751-5d8a404d73a9",
    "_uuid": "0bca9739b82d5d51e1229243e03ea1b6db35c17e"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classiﬁcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n",
    "\n",
    "If you're not familiar with naive bayes and bag of words matrices, I've made a preview available of one of fast.ai's upcoming *Practical Machine Learning* course videos, which introduces this topic. Here is a link to the section of the video which discusses this: [Naive Bayes video](https://youtu.be/37sFIak42Sc?t=3745)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "ef06cd19-66b6-46bc-bf45-184e12d3f7d4",
    "_uuid": "cca038ca9424a3f66e10262fc9129de807b5f855",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/toxicity_annotated_comments.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = pd.read_csv('data/toxicity_annotations.tsv',  sep='\\t')\n",
    "scores.drop_duplicates(subset='rev_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(scores, on='rev_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159686"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['year', 'logged_in', 'split', 'ns', 'sample', 'worker_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  toxicity  \\\n",
       "0   2232.0  This:NEWLINE_TOKEN:One can make an analogy in ...         0   \n",
       "1   4216.0  `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...         0   \n",
       "2   8953.0                          Elected or Electoral? JHK         0   \n",
       "3  26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...         0   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...         0   \n",
       "\n",
       "   toxicity_score  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             1.0  \n",
       "3             1.0  \n",
       "4             1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def mr_clean(comment):\n",
    "    comment = re.sub('NEWLINE_TOKEN', '', comment)\n",
    "    comment = re_tok.sub('', comment)             # remove punctuation\n",
    "    comment = re.sub('_', ' ', comment)\n",
    "    comment = re.sub( '\\s+', ' ', comment)\n",
    "    comment = comment.strip()\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(mr_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['toxic'] = df['toxicity_score'].apply(lambda x: int(x < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['toxicity', 'toxicity_score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>ThisOne can make an analogy in mathematical te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>Clarification for you and Zundarks right i sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral JHK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>This is such a fun entry DevotchkaI once had a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  toxic\n",
       "0   2232.0  ThisOne can make an analogy in mathematical te...      0\n",
       "1   4216.0  Clarification for you and Zundarks right i sho...      0\n",
       "2   8953.0                           Elected or Electoral JHK      0\n",
       "3  26547.0  This is such a fun entry DevotchkaI once had a...      0\n",
       "4  28959.0  Please relate the ozone hole to increases in c...      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df.toxic\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ea37597-02f7-43cf-ad16-a3d50aac1aba",
    "_uuid": "5c4c716de98a4b1c2ecc0e516e67813b4fc1473e"
   },
   "source": [
    "The length of the comments varies a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "fd3fe158-4d7f-4b30-ac15-42605240ea4f",
    "_uuid": "9c1a3f81397199fa250a2b642edc7fbc5f9f504e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374.31681707800749, 563.54040399226028, 5000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = X_train.comment.str.len()\n",
    "lens.mean(), lens.std(), lens.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8515824-b2dd-4c95-bbf9-dc74c80355db",
    "_uuid": "0151ab55887071aed82d297acb2c6545ed964c2b"
   },
   "source": [
    "We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143717"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15969"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "fdba531c-7ef2-4967-88e2-fc2b04f6f2ef",
    "_uuid": "1e1229f403225f1889c7a7b4fc9be90fda818af5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COMMENT = 'comment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "480780f1-00c0-4f9a-81e5-fc1932516a80",
    "_uuid": "f2e77e8e6df5e29b620c7a2a0add1438c35af932"
   },
   "source": [
    "## Building the model\n",
    "\n",
    "We'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "b7f11db7-5c12-4eb8-9f2d-0323d629fed9",
    "_uuid": "b043a3fb66c443fab0129e863c134ec813dadb87",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfdebf11-133c-4b12-8664-8bf64757d6cc",
    "_uuid": "941759df15c71d42853515e4d1006f4ab000ce75"
   },
   "source": [
    "It turns out that using TF-IDF gives even better priors than the binarized features used in the paper. I don't think this has been mentioned in any paper before, but it improves leaderboard score from 0.59 to 0.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "31ad6c98-d054-426c-b3bd-b3b18f52eb6f",
    "_uuid": "75f3f27d56fb2d7d539e65c292d9e77c92ceead3"
   },
   "outputs": [],
   "source": [
    "n = X_train.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "trn_term_doc = vec.fit_transform(X_train[COMMENT])\n",
    "test_term_doc = vec.transform(X_test[COMMENT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4cf3ec26-8237-452b-90c9-831cb0297955",
    "_uuid": "6d215bc460e64d88b08f501d5c5a67c290e40635"
   },
   "source": [
    "This creates a *sparse matrix* with only a small number of non-zero elements (*stored elements* in the representation  below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "4c7bdbcc-4451-4477-944c-772e99bac777",
    "_uuid": "8816cc35f66b9fed9c12978fbdef5bb68fae10f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<143717x355752 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 12662305 stored elements in Compressed Sparse Row format>,\n",
       " <15969x355752 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1366860 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc, test_term_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59131479-a861-4f46-add9-b2af09a51976",
    "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"
   },
   "source": [
    "Here's the basic naive bayes feature equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "45fc6070-ba13-455b-9274-5c2611e2809c",
    "_uuid": "8b277f01cecd575ed4fcae2e630c0dd8ce979793",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "2299d24b-5515-4d37-92d9-e7f6b16a290a",
    "_uuid": "926eaa2e40e588f4ef2b86e0a28f8e575c9ed5f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = trn_term_doc\n",
    "test_x = test_term_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c0b494ac-0dfc-4faa-a909-0a6d7696d1fc",
    "_uuid": "dc5cafeab86d17ac4f036d58658437636a885a87"
   },
   "source": [
    "Fit a model for one dependent at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "b756c889-a383-4952-9ee9-eca79fd3454f",
    "_uuid": "8652ab2f5f84e77fa395252be9b60be1e44fd583",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "33fd5f8c-adfc-45a1-9fde-1769a0993e76",
    "_uuid": "0fa103b5406aabdc36ea9ef21612d343e4982fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic']\n",
    "preds = np.zeros((len(X_test), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_mdl(X_train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28089634788067197"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "y_true = X_test.as_matrix(columns=['toxic'])\n",
    "log_loss(y_true, preds[:,0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89805247667355503"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, preds[:, 0].round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "1c345d02-b768-491c-8c03-8c3459a552a8",
    "_uuid": "adbbfb0156952a6a43833e337b8a418ccac257aa",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
