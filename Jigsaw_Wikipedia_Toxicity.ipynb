{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction.\n",
    "\n",
    "These are some (modest) attempts at participating in Jigsaw's toxic comments classification problem. For now, I am not using any external data, only the training data given (which is limiting as it's a tiny dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col='id')\n",
    "df_test = pd.read_csv('data/test.csv', index_col='id')\n",
    "# One test input is missing data, so we will just replace it by an empty string.\n",
    "df_test['comment_text'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22256635</th>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27450690</th>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54037174</th>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77493077</th>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79357270</th>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comment_text  toxic  \\\n",
       "id                                                                   \n",
       "22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "          severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                              \n",
       "22256635             0        0       0       0              0  \n",
       "27450690             0        0       0       0              0  \n",
       "54037174             0        0       0       0              0  \n",
       "77493077             0        0       0       0              0  \n",
       "79357270             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95851"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_tokens = df.comment_text.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrases = gensim.models.phrases.Phrases(simple_tokens)\n",
    "tokenizer = gensim.models.phrases.Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_text = list(tokenizer[simple_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nonsense',\n",
       " 'kiss',\n",
       " 'off',\n",
       " 'geek',\n",
       " 'what',\n",
       " 'said',\n",
       " 'is',\n",
       " 'true',\n",
       " 'll',\n",
       " 'have',\n",
       " 'your',\n",
       " 'account',\n",
       " 'terminated']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_dict = gensim.corpora.dictionary.Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET_CLASSES = df.columns[1:]\n",
    "targets = df[TARGET_CLASSES].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 400)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWTkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c2114a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot([len(doc) for doc in tokenized_text], bins=100, kde=False, label='Number of tokens per comment.')\n",
    "plt.xlabel(\"Tokens in a comment\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim((0, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word2vec on comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.word2vec.Word2Vec(tokenized_text, window=5, size=100, min_count=2, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reference', 0.8087868094444275),\n",
       " ('citations', 0.7814565300941467),\n",
       " ('references', 0.7771475911140442),\n",
       " ('source', 0.7747196555137634),\n",
       " ('reliable_source', 0.7457165122032166),\n",
       " ('secondary_source', 0.7392337322235107),\n",
       " ('ref', 0.7367275357246399),\n",
       " ('sources', 0.7204996347427368),\n",
       " ('quote', 0.7049827575683594),\n",
       " ('cite', 0.6986434459686279)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dumb', 0.8715893626213074),\n",
       " ('pathetic', 0.8467798233032227),\n",
       " ('crazy', 0.8234038352966309),\n",
       " ('bullshit', 0.7920104265213013),\n",
       " ('fucking', 0.7908686995506287),\n",
       " ('fool', 0.7828363180160522),\n",
       " ('moron', 0.7735713720321655),\n",
       " ('dude', 0.7664137482643127),\n",
       " ('sick', 0.7639930248260498),\n",
       " ('arrogant', 0.7635257244110107)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('stupid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec-based based model.\n",
    "\n",
    "Aggregate word embeddings per comment (~ tf-idf weighted averaging), and use that as an input feature in a neural net with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.zeros((len(tokenized_text), word2vec.vector_size))\n",
    "for i, tokens in enumerate(tokenized_text):\n",
    "    tokens = [t for t in tokens if t in word2vec.wv.vocab]\n",
    "    if tokens:\n",
    "        features[i, :] = np.mean([word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(word2vec.vector_size,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/10\n",
      "86265/86265 [==============================] - 4s 47us/step - loss: 0.1027 - acc: 0.9677 - val_loss: 0.0826 - val_acc: 0.9700\n",
      "Epoch 2/10\n",
      "86265/86265 [==============================] - 3s 39us/step - loss: 0.0798 - acc: 0.9712 - val_loss: 0.0803 - val_acc: 0.9708\n",
      "Epoch 3/10\n",
      "86265/86265 [==============================] - 3s 39us/step - loss: 0.0783 - acc: 0.9716 - val_loss: 0.0804 - val_acc: 0.9711\n",
      "Epoch 4/10\n",
      "86265/86265 [==============================] - 3s 40us/step - loss: 0.0772 - acc: 0.9721 - val_loss: 0.0795 - val_acc: 0.9712\n",
      "Epoch 5/10\n",
      "86265/86265 [==============================] - 4s 44us/step - loss: 0.0768 - acc: 0.9723 - val_loss: 0.0787 - val_acc: 0.9711\n",
      "Epoch 6/10\n",
      "86265/86265 [==============================] - 3s 39us/step - loss: 0.0761 - acc: 0.9724 - val_loss: 0.0813 - val_acc: 0.9713\n",
      "Epoch 7/10\n",
      "86265/86265 [==============================] - 3s 40us/step - loss: 0.0756 - acc: 0.9725 - val_loss: 0.0770 - val_acc: 0.9715\n",
      "Epoch 8/10\n",
      "86265/86265 [==============================] - 3s 40us/step - loss: 0.0749 - acc: 0.9728 - val_loss: 0.0773 - val_acc: 0.9718\n",
      "Epoch 9/10\n",
      "86265/86265 [==============================] - 4s 43us/step - loss: 0.0742 - acc: 0.9730 - val_loss: 0.0764 - val_acc: 0.9723\n",
      "Epoch 10/10\n",
      "86265/86265 [==============================] - 3s 40us/step - loss: 0.0740 - acc: 0.9733 - val_loss: 0.0748 - val_acc: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13615c828>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features, targets, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential models\n",
    "\n",
    "Simply averaging embeddings across all terms in a comment loses interactions that can occur between words, and the importance of their position. Because of this, we will now experiment with position-aware models: LSTM and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: shifting indices by 1 as index 0 will be used for padding.\n",
    "docs = [[idx + 1 for idx in corpus_dict.doc2idx(doc)]  for doc in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(docs, maxlen=MAX_SEQ_LEN, truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136735"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx = max(c for d in docs for c in d)\n",
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.array([np.random.normal(size=word2vec.vector_size)]+ # for the '0' padding word\n",
    "                      [word2vec.wv[corpus_dict[idx]]\n",
    "                      if corpus_dict[idx] in word2vec.wv.vocab\n",
    "                      else np.random.normal(size=word2vec.vector_size)\n",
    "                      for idx in range(max_idx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (WIP)\n",
    "\n",
    "We use an LSTM with an embedding layer, and use padded sequences as an input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Identity matrix initializer can only be used for 2D square matrices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d323cd6c67a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'identity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTARGET_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    487\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m    575\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# set or validate state_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1720\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m             constraint=self.recurrent_constraint)\n\u001b[0m\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[1;32m    398\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             raise ValueError('Identity matrix initializer can only be used '\n\u001b[0m\u001b[1;32m    275\u001b[0m                              'for 2D square matrices.')\n\u001b[1;32m    276\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Identity matrix initializer can only be used for 2D square matrices."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_idx + 1, word2vec.vector_size, input_length=MAX_SEQ_LEN))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(256, recurrent_initializer='identity'))\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "model.compile('rmsprop', 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(padded_docs, targets, batch_size=256, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPool1D, Flatten, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_idx + 1, word2vec.vector_size, input_length=MAX_SEQ_LEN))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(52, 5, padding='same',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(128, 3, padding='same',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.02)))\n",
    "model.compile(Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/5\n",
      "86265/86265 [==============================] - 65s 754us/step - loss: 0.5531 - acc: 0.9632 - val_loss: 0.2552 - val_acc: 0.9624\n",
      "Epoch 2/5\n",
      "86265/86265 [==============================] - 65s 755us/step - loss: 0.1059 - acc: 0.9774 - val_loss: 0.1616 - val_acc: 0.9625\n",
      "Epoch 3/5\n",
      "86265/86265 [==============================] - 65s 755us/step - loss: 0.0837 - acc: 0.9809 - val_loss: 0.1251 - val_acc: 0.9660\n",
      "Epoch 4/5\n",
      "86265/86265 [==============================] - 66s 769us/step - loss: 0.0748 - acc: 0.9823 - val_loss: 0.1000 - val_acc: 0.9749\n",
      "Epoch 5/5\n",
      "86265/86265 [==============================] - 66s 761us/step - loss: 0.0683 - acc: 0.9837 - val_loss: 0.0950 - val_acc: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c26fc50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, targets, batch_size=512, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/10\n",
      "86265/86265 [==============================] - 65s 756us/step - loss: 0.0639 - acc: 0.9847 - val_loss: 0.0957 - val_acc: 0.9769\n",
      "Epoch 2/10\n",
      "86265/86265 [==============================] - 66s 765us/step - loss: 0.0615 - acc: 0.9851 - val_loss: 0.0915 - val_acc: 0.9777\n",
      "Epoch 3/10\n",
      "86265/86265 [==============================] - 69s 805us/step - loss: 0.0586 - acc: 0.9856 - val_loss: 0.0922 - val_acc: 0.9771\n",
      "Epoch 4/10\n",
      "86265/86265 [==============================] - 66s 768us/step - loss: 0.0569 - acc: 0.9859 - val_loss: 0.0920 - val_acc: 0.9771\n",
      "Epoch 5/10\n",
      "86265/86265 [==============================] - 65s 753us/step - loss: 0.0549 - acc: 0.9865 - val_loss: 0.0973 - val_acc: 0.9768\n",
      "Epoch 6/10\n",
      "86265/86265 [==============================] - 68s 792us/step - loss: 0.0535 - acc: 0.9866 - val_loss: 0.0930 - val_acc: 0.9770\n",
      "Epoch 7/10\n",
      "86265/86265 [==============================] - 66s 769us/step - loss: 0.0526 - acc: 0.9867 - val_loss: 0.0945 - val_acc: 0.9769\n",
      "Epoch 8/10\n",
      "86265/86265 [==============================] - 67s 777us/step - loss: 0.0520 - acc: 0.9868 - val_loss: 0.0936 - val_acc: 0.9772\n",
      "Epoch 9/10\n",
      "86265/86265 [==============================] - 66s 762us/step - loss: 0.0511 - acc: 0.9870 - val_loss: 0.0907 - val_acc: 0.9774\n",
      "Epoch 10/10\n",
      "86265/86265 [==============================] - 65s 758us/step - loss: 0.0504 - acc: 0.9872 - val_loss: 0.0919 - val_acc: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x173d998d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, targets, batch_size=512, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comment_to_sequential_input(comment):\n",
    "    tokens = tokenizer[gensim.utils.simple_preprocess(comment)]\n",
    "    t_ids = [corpus_dict.token2id[t] + 1 for t in tokens if t in word2vec.wv.vocab and t in corpus_dict.token2id]\n",
    "    return keras.preprocessing.sequence.pad_sequences([t_ids], maxlen=MAX_SEQ_LEN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 4.94%\n",
      "severe_toxic: 0.82%\n",
      "obscene: 1.76%\n",
      "threat: 0.92%\n",
      "insult: 1.89%\n",
      "identity_hate: 1.79%\n"
     ]
    }
   ],
   "source": [
    "test_input = [comment_to_sequential_input(\"Why are we having all these people from shithole countries come here?\").reshape(1, -1)]\n",
    "for target_class, score in zip(TARGET_CLASSES, model.predict(test_input)[0]):\n",
    "    print(\"{}: {:.2f}%\".format(target_class, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_inputs = np.array([comment_to_sequential_input(doc) for doc in df_test.comment_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_outputs = model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.42359537,  0.02407806,  0.09000999,  0.01083746,  0.10032999,\n",
       "        0.0244122 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df = df_test.reset_index()[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, target_class in enumerate(TARGET_CLASSES):\n",
    "    output_df[target_class] = test_outputs[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207776000712</td>\n",
       "      <td>0.948848</td>\n",
       "      <td>0.019023</td>\n",
       "      <td>0.383647</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>0.281466</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>\"This is ridiculous. \"\"It makes Iranians look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>473421599211</td>\n",
       "      <td>0.684538</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.144640</td>\n",
       "      <td>0.007382</td>\n",
       "      <td>0.127685</td>\n",
       "      <td>0.017104</td>\n",
       "      <td>OK, calm down. I only did it once so how the h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75859090464</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.638960</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>0.996361</td>\n",
       "      <td>0.285101</td>\n",
       "      <td>no fucking shit, dick head.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962041181389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.396993</td>\n",
       "      <td>0.999879</td>\n",
       "      <td>0.044877</td>\n",
       "      <td>0.998712</td>\n",
       "      <td>0.227803</td>\n",
       "      <td>Sallary controversy  \\n\\nArzel, watch your bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>511198810846</td>\n",
       "      <td>0.874407</td>\n",
       "      <td>0.020444</td>\n",
       "      <td>0.234701</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>0.173478</td>\n",
       "      <td>0.014323</td>\n",
       "      <td>\"\\n\\nWhy in the FVCK would you give a \"\"love\"\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>669469110204</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>0.992761</td>\n",
       "      <td>0.078995</td>\n",
       "      <td>0.947661</td>\n",
       "      <td>0.092362</td>\n",
       "      <td>HEY ALAN!\\nYOU FUCKING SUCK MY DICK, you gaybo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>643477166986</td>\n",
       "      <td>0.690494</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>::::I replied here since this is where the dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>390060787074</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.157694</td>\n",
       "      <td>0.968267</td>\n",
       "      <td>0.020527</td>\n",
       "      <td>0.897413</td>\n",
       "      <td>0.074419</td>\n",
       "      <td>Screw you to goddamn hippies!!\\n\\nWikipedia Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75685472898</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.379813</td>\n",
       "      <td>0.995993</td>\n",
       "      <td>0.019506</td>\n",
       "      <td>0.976859</td>\n",
       "      <td>0.155629</td>\n",
       "      <td>Text Edit\\n\\nFuck. Shit. Cunt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>946083055427</td>\n",
       "      <td>0.991652</td>\n",
       "      <td>0.098146</td>\n",
       "      <td>0.795065</td>\n",
       "      <td>0.014297</td>\n",
       "      <td>0.648909</td>\n",
       "      <td>0.061169</td>\n",
       "      <td>=FUCK U CUNT=</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  207776000712  0.948848      0.019023  0.383647  0.002692  0.281466   \n",
       "1  473421599211  0.684538      0.020371  0.144640  0.007382  0.127685   \n",
       "2   75859090464  0.999999      0.638960  0.999577  0.030429  0.996361   \n",
       "3  962041181389  1.000000      0.396993  0.999879  0.044877  0.998712   \n",
       "4  511198810846  0.874407      0.020444  0.234701  0.007127  0.173478   \n",
       "5  669469110204  0.999987      0.117371  0.992761  0.078995  0.947661   \n",
       "6  643477166986  0.690494      0.007340  0.070389  0.004534  0.066467   \n",
       "7  390060787074  0.999694      0.157694  0.968267  0.020527  0.897413   \n",
       "8   75685472898  0.999981      0.379813  0.995993  0.019506  0.976859   \n",
       "9  946083055427  0.991652      0.098146  0.795065  0.014297  0.648909   \n",
       "\n",
       "   identity_hate                                       comment_text  \n",
       "0       0.013564  \"This is ridiculous. \"\"It makes Iranians look ...  \n",
       "1       0.017104  OK, calm down. I only did it once so how the h...  \n",
       "2       0.285101                        no fucking shit, dick head.  \n",
       "3       0.227803  Sallary controversy  \\n\\nArzel, watch your bac...  \n",
       "4       0.014323  \"\\n\\nWhy in the FVCK would you give a \"\"love\"\"...  \n",
       "5       0.092362  HEY ALAN!\\nYOU FUCKING SUCK MY DICK, you gaybo...  \n",
       "6       0.008397  ::::I replied here since this is where the dis...  \n",
       "7       0.074419  Screw you to goddamn hippies!!\\n\\nWikipedia Su...  \n",
       "8       0.155629                     Text Edit\\n\\nFuck. Shit. Cunt.  \n",
       "9       0.061169                                      =FUCK U CUNT=  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[output_df.toxic > 0.5].sample(10, random_state=0).merge(df_test.reset_index(), on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('submissions/cnn_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(word2vec.vector_size))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_embedding(tokens):\n",
    "    embeddings = [word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens if t in word2vec.wv.vocab]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "\n",
    "def text_to_embedding(text):\n",
    "    return tokens_to_embedding(tokenizer[gensim.utils.simple_preprocess(text)])\n",
    "\n",
    "text = 'hello moroccan friend is just a regular message without any insults'\n",
    "model.predict(text_to_embedding(text).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tokens = tokenizer[df_test.comment_text.apply(gensim.utils.simple_preprocess)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = [tokens_to_embedding(tokens) for tokens in test_tokens]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
