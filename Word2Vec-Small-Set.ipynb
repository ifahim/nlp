{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction.\n",
    "\n",
    "These are some (modest) attempts at participating in Jigsaw's toxic comments classification problem. For now, I am not using any external data, only the training data given (which is limiting as it's a tiny dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/toxicity_annotated_comments_unanimous.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>603474.0</td>\n",
       "      <td>`:Here I cannot answer your questions.  I did ...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>808576.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENIn fact, I've just d...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>986363.0</td>\n",
       "      <td>16:13 1 Jun 2003 (UTC)</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433843.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN:I replied on .</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rev_id                                            comment  year  \\\n",
       "0   527004.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2002   \n",
       "1   603474.0  `:Here I cannot answer your questions.  I did ...  2003   \n",
       "2   808576.0  NEWLINE_TOKENNEWLINE_TOKENIn fact, I've just d...  2003   \n",
       "3   986363.0                             16:13 1 Jun 2003 (UTC)  2003   \n",
       "4  1433843.0        NEWLINE_TOKENNEWLINE_TOKEN:I replied on .    2003   \n",
       "\n",
       "   logged_in       ns  sample  split  \n",
       "0       True     user  random   test  \n",
       "1       True  article  random  train  \n",
       "2       True     user  random    dev  \n",
       "3       True  article  random  train  \n",
       "4       True     user  random  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = pd.read_csv('data/toxicity_annotations_unanimous.tsv',  sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>3802</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>3942</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>590</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>481</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>3944</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>4172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>2419</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>3323</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>2851</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>4175</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rev_id  worker_id  toxicity  toxicity_score\n",
       "0  527004.0       3802         0             0.0\n",
       "1  527004.0       3942         0             0.0\n",
       "2  527004.0        590         0             0.0\n",
       "3  527004.0        481         0             0.0\n",
       "4  527004.0       3944         0             0.0\n",
       "5  527004.0       4172         0             0.0\n",
       "6  527004.0       2419         0             0.0\n",
       "7  527004.0       3323         0             0.0\n",
       "8  527004.0       2851         0             0.0\n",
       "9  527004.0       4175         0             0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores.drop_duplicates(subset='rev_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(scores, on='rev_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>3802</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>603474.0</td>\n",
       "      <td>`:Here I cannot answer your questions.  I did ...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3278</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>808576.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENIn fact, I've just d...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>809</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>986363.0</td>\n",
       "      <td>16:13 1 Jun 2003 (UTC)</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1081</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433843.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN:I replied on .</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>2794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1988528.0</td>\n",
       "      <td>NEWLINE_TOKENHA AH AHA HAHAHAHHAHAHAHHAHAH</td>\n",
       "      <td>2003</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3374</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1988988.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN Reques...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2078042.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>2059</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2747887.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENOh - well - if you p...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>1993</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2850252.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>561</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rev_id                                            comment  year  \\\n",
       "0   527004.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2002   \n",
       "1   603474.0  `:Here I cannot answer your questions.  I did ...  2003   \n",
       "2   808576.0  NEWLINE_TOKENNEWLINE_TOKENIn fact, I've just d...  2003   \n",
       "3   986363.0                             16:13 1 Jun 2003 (UTC)  2003   \n",
       "4  1433843.0        NEWLINE_TOKENNEWLINE_TOKEN:I replied on .    2003   \n",
       "5  1988528.0         NEWLINE_TOKENHA AH AHA HAHAHAHHAHAHAHHAHAH  2003   \n",
       "6  1988988.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN Reques...  2003   \n",
       "7  2078042.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2003   \n",
       "8  2747887.0  NEWLINE_TOKENNEWLINE_TOKENOh - well - if you p...  2004   \n",
       "9  2850252.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2004   \n",
       "\n",
       "   logged_in       ns  sample  split  worker_id  toxicity  toxicity_score  \n",
       "0       True     user  random   test       3802         0             0.0  \n",
       "1       True  article  random  train       3278         0             0.0  \n",
       "2       True     user  random    dev        809         0             0.0  \n",
       "3       True  article  random  train       1081         0             0.0  \n",
       "4       True     user  random  train       2794         0             0.0  \n",
       "5      False     user  random  train       3374         0             0.0  \n",
       "6       True     user  random  train       1028         0             0.0  \n",
       "7       True     user  random  train       2059         0             0.0  \n",
       "8       True     user  random   test       1993         0             0.0  \n",
       "9       True     user  random   test        561         0             0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['year', 'logged_in', 'split', 'ns', 'sample'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527004.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...</td>\n",
       "      <td>3802</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>603474.0</td>\n",
       "      <td>`:Here I cannot answer your questions.  I did ...</td>\n",
       "      <td>3278</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>808576.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENIn fact, I've just d...</td>\n",
       "      <td>809</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>986363.0</td>\n",
       "      <td>16:13 1 Jun 2003 (UTC)</td>\n",
       "      <td>1081</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433843.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN:I replied on .</td>\n",
       "      <td>2794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1988528.0</td>\n",
       "      <td>NEWLINE_TOKENHA AH AHA HAHAHAHHAHAHAHHAHAH</td>\n",
       "      <td>3374</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1988988.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN Reques...</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2078042.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...</td>\n",
       "      <td>2059</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2747887.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENOh - well - if you p...</td>\n",
       "      <td>1993</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2850252.0</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...</td>\n",
       "      <td>561</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rev_id                                            comment  worker_id  \\\n",
       "0   527004.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...       3802   \n",
       "1   603474.0  `:Here I cannot answer your questions.  I did ...       3278   \n",
       "2   808576.0  NEWLINE_TOKENNEWLINE_TOKENIn fact, I've just d...        809   \n",
       "3   986363.0                             16:13 1 Jun 2003 (UTC)       1081   \n",
       "4  1433843.0        NEWLINE_TOKENNEWLINE_TOKEN:I replied on .         2794   \n",
       "5  1988528.0         NEWLINE_TOKENHA AH AHA HAHAHAHHAHAHAHHAHAH       3374   \n",
       "6  1988988.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN Reques...       1028   \n",
       "7  2078042.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...       2059   \n",
       "8  2747887.0  NEWLINE_TOKENNEWLINE_TOKENOh - well - if you p...       1993   \n",
       "9  2850252.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...        561   \n",
       "\n",
       "   toxicity  toxicity_score  \n",
       "0         0             0.0  \n",
       "1         0             0.0  \n",
       "2         0             0.0  \n",
       "3         0             0.0  \n",
       "4         0             0.0  \n",
       "5         0             0.0  \n",
       "6         0             0.0  \n",
       "7         0             0.0  \n",
       "8         0             0.0  \n",
       "9         0             0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(lambda x: re.sub('NEWLINE_TOKEN', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxic'] = df['toxicity_score'].apply(lambda x: int(x < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['toxicity', 'toxicity_score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['toxic'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3581</th>\n",
       "      <td>699660419.0</td>\n",
       "      <td>what ever you fuggin fagQuestion how did you k...</td>\n",
       "      <td>642</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>459810020.0</td>\n",
       "      <td>==You absolute spastic==I am right, Username72...</td>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>145045349.0</td>\n",
       "      <td>Racist?  Are you crazy?  I thought that you we...</td>\n",
       "      <td>695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>516983664.0</td>\n",
       "      <td>Intro should say that the Tea Party are a whit...</td>\n",
       "      <td>877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>140385226.0</td>\n",
       "      <td>it should be biased against those soulless fuc...</td>\n",
       "      <td>340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>528187898.0</td>\n",
       "      <td>==message to attackers==Leave him alone. what ...</td>\n",
       "      <td>1393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>528427363.0</td>\n",
       "      <td>this is your last warning stop violating multi...</td>\n",
       "      <td>3942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>129871433.0</td>\n",
       "      <td>YOU SUCK YOU SUCK YOU SUCK YOU SUCK YOU SUCK Y...</td>\n",
       "      <td>3330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>531565847.0</td>\n",
       "      <td>`::::::::::::::::::::::::I've told you, in goo...</td>\n",
       "      <td>597</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>532661527.0</td>\n",
       "      <td>==TIME?==Talk about unprofessional garbage. Th...</td>\n",
       "      <td>3720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rev_id                                            comment  \\\n",
       "3581  699660419.0  what ever you fuggin fagQuestion how did you k...   \n",
       "2507  459810020.0  ==You absolute spastic==I am right, Username72...   \n",
       "846   145045349.0  Racist?  Are you crazy?  I thought that you we...   \n",
       "2789  516983664.0  Intro should say that the Tea Party are a whit...   \n",
       "824   140385226.0  it should be biased against those soulless fuc...   \n",
       "2829  528187898.0  ==message to attackers==Leave him alone. what ...   \n",
       "2831  528427363.0  this is your last warning stop violating multi...   \n",
       "778   129871433.0  YOU SUCK YOU SUCK YOU SUCK YOU SUCK YOU SUCK Y...   \n",
       "2845  531565847.0  `::::::::::::::::::::::::I've told you, in goo...   \n",
       "2852  532661527.0  ==TIME?==Talk about unprofessional garbage. Th...   \n",
       "\n",
       "      worker_id  toxic  \n",
       "3581        642      1  \n",
       "2507        784      1  \n",
       "846         695      1  \n",
       "2789        877      1  \n",
       "824         340      1  \n",
       "2829       1393      1  \n",
       "2831       3942      1  \n",
       "778        3330      1  \n",
       "2845        597      1  \n",
       "2852       3720      1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_tokens = df.comment.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrases = gensim.models.phrases.Phrases(simple_tokens)\n",
    "tokenizer = gensim.models.phrases.Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_text = list(tokenizer[simple_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'ever',\n",
       " 'you',\n",
       " 'fuggin',\n",
       " 'fagquestion',\n",
       " 'how',\n",
       " 'did',\n",
       " 'you',\n",
       " 'know',\n",
       " 'they',\n",
       " 'were',\n",
       " 'not',\n",
       " 'mine']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_dict = gensim.corpora.dictionary.Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rev_id', 'comment', 'worker_id', 'toxic'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASSES = ['toxic']\n",
    "targets = df[TARGET_CLASSES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ..., \n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 400)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUCRJXRgokqQu/j9O5TN/JHrxRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120bc4e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot([len(doc) for doc in tokenized_text], bins=100, kde=False, label='Number of tokens per comment.')\n",
    "plt.xlabel(\"Tokens in a comment\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim((0, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word2vec on comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.word2vec.Word2Vec(tokenized_text, window=5, size=100, min_count=2, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('been', 0.999602735042572),\n",
       " ('the_same', 0.9995899200439453),\n",
       " ('am', 0.9995876550674438),\n",
       " ('very', 0.9995816349983215),\n",
       " ('time', 0.9995666146278381),\n",
       " ('before', 0.9995635747909546),\n",
       " ('section', 0.9995619058609009),\n",
       " ('another', 0.9995591044425964),\n",
       " ('editing', 0.9995579123497009),\n",
       " ('has', 0.9995574951171875)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('always', 0.9978344440460205),\n",
       " ('best', 0.9975923895835876),\n",
       " ('do_not', 0.9975626468658447),\n",
       " ('christopher', 0.9975571632385254),\n",
       " ('hunter', 0.9975089430809021),\n",
       " ('are_foreigners', 0.9974634647369385),\n",
       " ('world', 0.9974256753921509),\n",
       " ('history', 0.9973925948143005),\n",
       " ('sa_oi', 0.997319221496582),\n",
       " ('through', 0.9973117709159851)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('fuck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec-based based model.\n",
    "\n",
    "Aggregate word embeddings per comment (~ tf-idf weighted averaging), and use that as an input feature in a neural net with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.zeros((len(tokenized_text), word2vec.vector_size))\n",
    "for i, tokens in enumerate(tokenized_text):\n",
    "    tokens = [t for t in tokens if t in word2vec.wv.vocab]\n",
    "    if tokens:\n",
    "        features[i, :] = np.mean([word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3582, 100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3582, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TARGET_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(word2vec.vector_size,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3223 samples, validate on 359 samples\n",
      "Epoch 1/10\n",
      "3223/3223 [==============================] - 1s 161us/step - loss: 0.2939 - acc: 0.9469 - val_loss: 0.0640 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "3223/3223 [==============================] - 0s 51us/step - loss: 0.2071 - acc: 0.9469 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "3223/3223 [==============================] - 0s 49us/step - loss: 0.2090 - acc: 0.9469 - val_loss: 0.0647 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "3223/3223 [==============================] - 0s 52us/step - loss: 0.2063 - acc: 0.9469 - val_loss: 0.0540 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "3223/3223 [==============================] - 0s 48us/step - loss: 0.2070 - acc: 0.9469 - val_loss: 0.0628 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "3223/3223 [==============================] - 0s 51us/step - loss: 0.2051 - acc: 0.9469 - val_loss: 0.0343 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "3223/3223 [==============================] - 0s 48us/step - loss: 0.2063 - acc: 0.9469 - val_loss: 0.0614 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "3223/3223 [==============================] - 0s 52us/step - loss: 0.2043 - acc: 0.9469 - val_loss: 0.0714 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "3223/3223 [==============================] - 0s 48us/step - loss: 0.2064 - acc: 0.9469 - val_loss: 0.0475 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "3223/3223 [==============================] - 0s 51us/step - loss: 0.2043 - acc: 0.9469 - val_loss: 0.0711 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122171e10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features, targets, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential models\n",
    "\n",
    "Simply averaging embeddings across all terms in a comment loses interactions that can occur between words, and the importance of their position. Because of this, we will now experiment with position-aware models: LSTM and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: shifting indices by 1 as index 0 will be used for padding.\n",
    "docs = [[idx + 1 for idx in corpus_dict.doc2idx(doc)]  for doc in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(docs, maxlen=MAX_SEQ_LEN, truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9957"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx = max(c for d in docs for c in d)\n",
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.array([np.random.normal(size=word2vec.vector_size)]+ # for the '0' padding word\n",
    "                      [word2vec.wv[corpus_dict[idx]]\n",
    "                      if corpus_dict[idx] in word2vec.wv.vocab\n",
    "                      else np.random.normal(size=word2vec.vector_size)\n",
    "                      for idx in range(max_idx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPool1D, Flatten, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_idx + 1, word2vec.vector_size, input_length=MAX_SEQ_LEN))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(52, 5, padding='same',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(128, 3, padding='same',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.02)))\n",
    "model.compile(Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3223 samples, validate on 359 samples\n",
      "Epoch 1/20\n",
      "3223/3223 [==============================] - 2s 701us/step - loss: 1.7133 - acc: 0.9153 - val_loss: 1.8461 - val_acc: 1.0000\n",
      "Epoch 2/20\n",
      "3223/3223 [==============================] - 1s 390us/step - loss: 1.5117 - acc: 0.9460 - val_loss: 1.6800 - val_acc: 1.0000\n",
      "Epoch 3/20\n",
      "3223/3223 [==============================] - 1s 380us/step - loss: 1.3427 - acc: 0.9485 - val_loss: 1.5959 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      "3223/3223 [==============================] - 1s 400us/step - loss: 1.1970 - acc: 0.9497 - val_loss: 1.4288 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "3223/3223 [==============================] - 1s 415us/step - loss: 1.0619 - acc: 0.9510 - val_loss: 1.2637 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "3223/3223 [==============================] - 1s 431us/step - loss: 0.9348 - acc: 0.9572 - val_loss: 1.1461 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "3223/3223 [==============================] - 1s 394us/step - loss: 0.8233 - acc: 0.9594 - val_loss: 0.9982 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "3223/3223 [==============================] - 1s 382us/step - loss: 0.7146 - acc: 0.9680 - val_loss: 0.8504 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "3223/3223 [==============================] - 1s 435us/step - loss: 0.6131 - acc: 0.9755 - val_loss: 0.7160 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "3223/3223 [==============================] - 1s 382us/step - loss: 0.5319 - acc: 0.9832 - val_loss: 0.5990 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "3223/3223 [==============================] - 1s 380us/step - loss: 0.4548 - acc: 0.9857 - val_loss: 0.5127 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "3223/3223 [==============================] - 1s 374us/step - loss: 0.3899 - acc: 0.9926 - val_loss: 0.4476 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "3223/3223 [==============================] - 1s 384us/step - loss: 0.3351 - acc: 0.9929 - val_loss: 0.3974 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "3223/3223 [==============================] - 1s 436us/step - loss: 0.2876 - acc: 0.9963 - val_loss: 0.3567 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "3223/3223 [==============================] - 1s 393us/step - loss: 0.2482 - acc: 0.9978 - val_loss: 0.3244 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "3223/3223 [==============================] - 1s 392us/step - loss: 0.2140 - acc: 0.9988 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "3223/3223 [==============================] - 1s 401us/step - loss: 0.1859 - acc: 0.9988 - val_loss: 0.2758 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "3223/3223 [==============================] - 1s 396us/step - loss: 0.1606 - acc: 0.9991 - val_loss: 0.2581 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "3223/3223 [==============================] - 1s 385us/step - loss: 0.1391 - acc: 0.9997 - val_loss: 0.2392 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "3223/3223 [==============================] - 1s 400us/step - loss: 0.1213 - acc: 0.9997 - val_loss: 0.2266 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1240cee48>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, targets, batch_size=512, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comment_to_sequential_input(comment):\n",
    "    tokens = tokenizer[gensim.utils.simple_preprocess(comment)]\n",
    "    t_ids = [corpus_dict.token2id[t] + 1 for t in tokens if t in word2vec.wv.vocab and t in corpus_dict.token2id]\n",
    "    return keras.preprocessing.sequence.pad_sequences([t_ids], maxlen=MAX_SEQ_LEN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 11.80%\n"
     ]
    }
   ],
   "source": [
    "test_input = [comment_to_sequential_input(\"Why are we having all these people from shithole countries come here?\").reshape(1, -1)]\n",
    "for target_class, score in zip(TARGET_CLASSES, model.predict(test_input)[0]):\n",
    "    print(\"{}: {:.2f}%\".format(target_class, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1180065]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df.iloc[0].comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 13.14%\n"
     ]
    }
   ],
   "source": [
    "test_input = [comment_to_sequential_input(test_text).reshape(1, -1)]\n",
    "for target_class, score in zip(TARGET_CLASSES, model.predict(test_input)[0]):\n",
    "    print(\"{}: {:.2f}%\".format(target_class, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text = \"Now is the time for all good persons to come to the aid of their country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 11.36%\n"
     ]
    }
   ],
   "source": [
    "test_input = [comment_to_sequential_input(test_text).reshape(1, -1)]\n",
    "for target_class, score in zip(TARGET_CLASSES, model.predict(test_input)[0]):\n",
    "    print(\"{}: {:.2f}%\".format(target_class, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_inputs = np.array([comment_to_sequential_input(doc) for doc in df_test.comment_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = model.predict_classes(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df = df_test.reset_index()[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, target_class in enumerate(TARGET_CLASSES):\n",
    "    output_df[target_class] = test_outputs[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[output_df.toxic > 0.5].sample(10, random_state=0).merge(df_test.reset_index(), on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df.to_csv('submissions/cnn_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(word2vec.vector_size))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_embedding(tokens):\n",
    "    embeddings = [word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens if t in word2vec.wv.vocab]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "\n",
    "def text_to_embedding(text):\n",
    "    return tokens_to_embedding(tokenizer[gensim.utils.simple_preprocess(text)])\n",
    "\n",
    "text = 'hello moroccan friend is just a regular message without any insults'\n",
    "model.predict(text_to_embedding(text).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tokens = tokenizer[df_test.comment_text.apply(gensim.utils.simple_preprocess)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = [tokens_to_embedding(tokens) for tokens in test_tokens]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
