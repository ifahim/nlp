{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction.\n",
    "\n",
    "These are some (modest) attempts at participating in Jigsaw's toxic comments classification problem. For now, I am not using any external data, only the training data given (which is limiting as it's a tiny dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import string\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/toxicity_annotated_comments.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  year  \\\n",
       "0   2232.0  This:NEWLINE_TOKEN:One can make an analogy in ...  2002   \n",
       "1   4216.0  `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...  2002   \n",
       "2   8953.0                          Elected or Electoral? JHK  2002   \n",
       "3  26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...  2002   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...  2002   \n",
       "\n",
       "   logged_in       ns  sample  split  \n",
       "0       True  article  random  train  \n",
       "1       True     user  random  train  \n",
       "2      False  article  random   test  \n",
       "3       True  article  random  train  \n",
       "4       True  article  random   test  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159686"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('../data/toxicity_annotations.tsv',  sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>723</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>3989</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>3341</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>1574</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>1508</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>772</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>4020</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  worker_id  toxicity  toxicity_score\n",
       "0  2232.0        723         0             0.0\n",
       "1  2232.0       4000         0             0.0\n",
       "2  2232.0       3989         0             1.0\n",
       "3  2232.0       3341         0             0.0\n",
       "4  2232.0       1574         0             1.0\n",
       "5  2232.0       1508         0             1.0\n",
       "6  2232.0        772         0             1.0\n",
       "7  2232.0        680         0             0.0\n",
       "8  2232.0        405         0             1.0\n",
       "9  2232.0       4020         1            -1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.drop_duplicates(subset='rev_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159686"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(scores, on='rev_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159686"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>723</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>2596</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1642</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35367.0</td>\n",
       "      <td>`:In an interpreted language your source code ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>1408</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37330.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI fixe...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>691</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37346.0</td>\n",
       "      <td>`If they are ``indisputable`` then why does th...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1108</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37675.0</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44377.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe co...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  year  \\\n",
       "0   2232.0  This:NEWLINE_TOKEN:One can make an analogy in ...  2002   \n",
       "1   4216.0  `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...  2002   \n",
       "2   8953.0                          Elected or Electoral? JHK  2002   \n",
       "3  26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...  2002   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...  2002   \n",
       "5  35367.0  `:In an interpreted language your source code ...  2002   \n",
       "6  37330.0  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI fixe...  2002   \n",
       "7  37346.0  `If they are ``indisputable`` then why does th...  2002   \n",
       "8  37675.0  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002   \n",
       "9  44377.0  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe co...  2002   \n",
       "\n",
       "   logged_in       ns  sample  split  worker_id  toxicity  toxicity_score  \n",
       "0       True  article  random  train        723         0             0.0  \n",
       "1       True     user  random  train        500         0             0.0  \n",
       "2      False  article  random   test       2596         0             1.0  \n",
       "3       True  article  random  train       1642         0             1.0  \n",
       "4       True  article  random   test        202         0             1.0  \n",
       "5       True  article  random    dev       1408         0             1.0  \n",
       "6       True  article  random  train        691         0             0.0  \n",
       "7       True  article  random  train       1108         0             0.0  \n",
       "8      False  article  random    dev        403         0             1.0  \n",
       "9       True  article  random  train       1927         0             2.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['year', 'logged_in', 'split', 'ns', 'sample', 'worker_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35367.0</td>\n",
       "      <td>`:In an interpreted language your source code ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37330.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI fixe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37346.0</td>\n",
       "      <td>`If they are ``indisputable`` then why does th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37675.0</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44377.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe co...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  toxicity  \\\n",
       "0   2232.0  This:NEWLINE_TOKEN:One can make an analogy in ...         0   \n",
       "1   4216.0  `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...         0   \n",
       "2   8953.0                          Elected or Electoral? JHK         0   \n",
       "3  26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...         0   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...         0   \n",
       "5  35367.0  `:In an interpreted language your source code ...         0   \n",
       "6  37330.0  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI fixe...         0   \n",
       "7  37346.0  `If they are ``indisputable`` then why does th...         0   \n",
       "8  37675.0  `-NEWLINE_TOKENThis is not ``creative``.  Thos...         0   \n",
       "9  44377.0  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe co...         0   \n",
       "\n",
       "   toxicity_score  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             1.0  \n",
       "3             1.0  \n",
       "4             1.0  \n",
       "5             1.0  \n",
       "6             0.0  \n",
       "7             0.0  \n",
       "8             1.0  \n",
       "9             2.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def mr_clean(comment):\n",
    "    comment = re.sub('NEWLINE_TOKEN', '', comment)\n",
    "    comment = re_tok.sub('', comment)             # remove punctuation\n",
    "    comment = re.sub('_', ' ', comment)\n",
    "    comment = re.sub( '\\s+', ' ', comment)\n",
    "    comment = comment.strip()\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(mr_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the words mapped into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the unique words \n",
    "from collections import Counter\n",
    "def find_all_unique_words_with_frequency(corpora):\n",
    "    bag_of_words_universe = []\n",
    "    for mysentence in corpora:\n",
    "        bag_of_words = mysentence.lower().split()\n",
    "        bag_of_words = [x.strip() if (len(x) < 30) else x[:30] for x in bag_of_words ]\n",
    "        bag_of_words_universe.extend(bag_of_words)\n",
    "    word_frequency = Counter(bag_of_words_universe)\n",
    "    return word_frequency\n",
    "\n",
    "corpora = df['comment'].values\n",
    "word_freq_dict = find_all_unique_words_with_frequency(corpora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = word_freq_dict.most_common()\n",
    "word_freq_df = pd.DataFrame(word_freq_df, columns = ['word', 'freq'], index = None)\n",
    "word_freq_df.to_csv('../data/word_frequency.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2 girls 1 cup', '2g1c', '4r5e', ..., 'zigabo', 'zipperhead',\n",
       "       'zoophilia'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a list of profane words \n",
    "badlist = pd.read_table('../data/profane_list.txt', header=None, comment = '#', encoding='utf-8')\n",
    "badlist = badlist.values.flatten()\n",
    "badlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Assuming that the data with freq 50 will be most frequent. \n",
    "most_frequent_words = [k for k,v in word_freq_dict.items() if v >= 25]\n",
    "print(most_frequent_words)\n",
    "least_frequent_words = [k for k,v in word_freq_dict.items() if v < 25]\n",
    "print(least_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "import time \n",
    "from datetime import datetime \n",
    "\n",
    "def divide_into_known_and_difficult(least_frequent_words, badlist):\n",
    "    difficult_words = []\n",
    "    known_words = []\n",
    "    d = enchant.Dict(\"en_US\")  \n",
    "    i = 0\n",
    "    tic = time.time()\n",
    "    for a_word in least_frequent_words:\n",
    "        if i% 100000 == 0:\n",
    "            toc = time.time()\n",
    "            print ('Number of commnets done = ', i, ',   time = ', toc-tic)\n",
    "            tic = toc\n",
    "        i += 1\n",
    "        if (d.check(a_word) or (a_word in badlist)):\n",
    "            known_words.append(a_word)\n",
    "        else:\n",
    "            difficult_words.append(a_word)\n",
    "    return known_words, difficult_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commnets done =  0 ,   time =  1.6689300537109375e-06\n",
      "Number of commnets done =  100000 ,   time =  6.963977336883545\n",
      "Number of commnets done =  200000 ,   time =  8.110761880874634\n"
     ]
    }
   ],
   "source": [
    "known_words, difficult_words = divide_into_known_and_difficult(least_frequent_words, badlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55532"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start creating the dictionary\n",
    "my_mapped_dictionary = {}\n",
    "for a_word in most_frequent_words:\n",
    "    my_mapped_dictionary[a_word] = a_word\n",
    "for a_word in known_words:\n",
    "    my_mapped_dictionary[a_word] = a_word\n",
    "len(my_mapped_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239509"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(difficult_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "def containsAny(seq, aset):\n",
    "    return bool(set(aset).intersection(seq))\n",
    "\n",
    "def find_mapping_for_difficult_word(myword, badlist):\n",
    "    myword_len = len(myword)\n",
    "    found_toxic_word = 0\n",
    "    #If the word is a number, replace with 'number'\n",
    "    if myword.isnumeric():\n",
    "        return ':number:'#, found_toxic_word\n",
    "    #Get fuzzy matches to the word\n",
    "    matches = process.extract(myword, badlist)\n",
    "    if myword_len == 30:\n",
    "        if matches[0][1] > 75: #Matching percent more than 75\n",
    "            found_toxic_word = 1\n",
    "            return matches[0][0]#, found_toxic_word            \n",
    "    #If string contains any of the following characters then try to map to the closest \n",
    "    if containsAny(myword, {'!', '@', '#', '$', '%', '^', '&', '*'}):\n",
    "        closest_match = matches[0]\n",
    "        for tup in matches[:5]:\n",
    "            if len(tup[0]) == len(myword):\n",
    "                closest_match = tup\n",
    "                break\n",
    "        new_word =   closest_match[0]\n",
    "        found_toxic_word = 1\n",
    "        return new_word#, found_toxic_word            \n",
    "    #If everything fails. Keep as is \n",
    "    return myword#, found_toxic_word\n",
    "\n",
    "def find_mapping_for_difficult_word_wrapper(difficult_words, badlist):\n",
    "    i = 0\n",
    "    tic = time.time()\n",
    "    for word in difficult_words:\n",
    "        if i% 1000 == 0:\n",
    "            toc = time.time()\n",
    "            print ('Number of commnets done = ', i, ',   time = ', toc-tic)\n",
    "            tic = toc\n",
    "        i += 1\n",
    "        new_word = find_mapping_for_difficult_word(word, badlist)\n",
    "        my_mapped_dictionary[word] = new_word\n",
    "    return (my_mapped_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment if you want to rerun this cell. It takes about 5 hours to run on my system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_mapped_dictionary = find_mapping_for_difficult_word_wrapper(difficult_words, badlist)\n",
    "# pickle.dump(my_mapped_dictionary, open( '../data/word_map_dictionary.pkl', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mapped_dictionary = pickle.load( open( '../data/word_map_dictionary.pkl', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'can': 'can',\n",
       " 'make': 'make',\n",
       " 'an': 'an',\n",
       " 'analogy': 'analogy',\n",
       " 'in': 'in',\n",
       " 'mathematical': 'mathematical',\n",
       " 'terms': 'terms',\n",
       " 'by': 'by',\n",
       " 'the': 'the',\n",
       " 'distribution': 'distribution',\n",
       " 'of': 'of',\n",
       " 'opinions': 'opinions',\n",
       " 'a': 'a',\n",
       " 'population': 'population',\n",
       " 'as': 'as',\n",
       " 'curve': 'curve',\n",
       " 'we': 'we',\n",
       " 'would': 'would',\n",
       " 'then': 'then',\n",
       " 'say': 'say',\n",
       " 'that': 'that',\n",
       " 'consensus': 'consensus',\n",
       " 'be': 'be',\n",
       " 'statement': 'statement',\n",
       " 'represents': 'represents',\n",
       " 'range': 'range',\n",
       " 'within': 'within',\n",
       " 'perhaps': 'perhaps',\n",
       " 'three': 'three',\n",
       " 'standard': 'standard',\n",
       " 'mean': 'mean',\n",
       " 'opinion': 'opinion',\n",
       " 'sounds': 'sounds',\n",
       " 'arbitrary': 'arbitrary',\n",
       " 'and': 'and',\n",
       " 'ad': 'ad',\n",
       " 'hoc': 'hoc',\n",
       " 'does': 'does',\n",
       " 'it': 'it',\n",
       " 'really': 'really',\n",
       " 'belong': 'belong',\n",
       " 'n': 'n',\n",
       " 'encyclopedia': 'encyclopedia',\n",
       " 'article': 'article',\n",
       " 'i': 'i',\n",
       " 'dont': 'dont',\n",
       " 'see': 'see',\n",
       " 'adds': 'adds',\n",
       " 'anything': 'anything',\n",
       " 'paragraph': 'paragraph',\n",
       " 'follows': 'follows',\n",
       " 'seems': 'seems',\n",
       " 'much': 'much',\n",
       " 'more': 'more',\n",
       " 'useful': 'useful',\n",
       " 'are': 'are',\n",
       " 'there': 'there',\n",
       " 'any': 'any',\n",
       " 'political': 'political',\n",
       " 'theorists': 'theorists',\n",
       " 'out': 'out',\n",
       " 'who': 'who',\n",
       " 'clarify': 'clarify',\n",
       " 'issues': 'issues',\n",
       " 'to': 'to',\n",
       " 'me': 'me',\n",
       " 'this': 'this',\n",
       " 'is': 'is',\n",
       " 'issue': 'issue',\n",
       " 'de': 'de',\n",
       " 'others': 'others',\n",
       " 'must': 'must',\n",
       " 'have': 'have',\n",
       " 'debated': 'debated',\n",
       " 'sr': 'sr',\n",
       " 'clarification': 'clarification',\n",
       " 'for': 'for',\n",
       " 'you': 'you',\n",
       " 'right': 'right',\n",
       " 'should': 'should',\n",
       " 'checked': 'checked',\n",
       " 'wikipedia': 'wikipedia',\n",
       " 'bugs': 'bugs',\n",
       " 'page': 'page',\n",
       " 'first': 'first',\n",
       " 'bug': 'bug',\n",
       " 'code': 'code',\n",
       " 'makes': 'makes',\n",
       " 'work': 'work',\n",
       " 'just': 'just',\n",
       " 'means': 'means',\n",
       " 'line': 'line',\n",
       " 'may': 'may',\n",
       " 'error': 'error',\n",
       " 'small': 'small',\n",
       " 'extra': 'extra',\n",
       " 'space': 'space',\n",
       " 'its': 'its',\n",
       " 'analogous': 'analogous',\n",
       " 'very': 'very',\n",
       " 'simplified': 'simplified',\n",
       " 'way': 'way',\n",
       " 'trying': 'trying',\n",
       " 'something': 'something',\n",
       " 'bold': 'bold',\n",
       " 'html': 'html',\n",
       " 'forgetting': 'forgetting',\n",
       " 'put': 'put',\n",
       " 'at': 'at',\n",
       " 'end': 'end',\n",
       " 'so': 'so',\n",
       " 'youd': 'youd',\n",
       " 'like': 'like',\n",
       " 'not': 'not',\n",
       " 'virus': 'virus',\n",
       " 'somebody': 'somebody',\n",
       " 'deliberately': 'deliberately',\n",
       " 'wrote': 'wrote',\n",
       " 'order': 'order',\n",
       " 'your': 'your',\n",
       " 'computer': 'computer',\n",
       " 'damage': 'damage',\n",
       " 'files': 'files',\n",
       " 'wont': 'wont',\n",
       " 'go': 'go',\n",
       " 'around': 'around',\n",
       " 'elected': 'elected',\n",
       " 'or': 'or',\n",
       " 'electoral': 'electoral',\n",
       " 'such': 'such',\n",
       " 'fun': 'fun',\n",
       " 'entry': 'entry',\n",
       " 'once': 'once',\n",
       " 'had': 'had',\n",
       " 'from': 'from',\n",
       " 'korea': 'korea',\n",
       " 'only': 'only',\n",
       " 'couldnt': 'couldnt',\n",
       " 'she': 'she',\n",
       " 'tell': 'tell',\n",
       " 'difference': 'difference',\n",
       " 'between': 'between',\n",
       " 'british': 'british',\n",
       " 'english': 'english',\n",
       " 'trouble': 'trouble',\n",
       " 'telling': 'telling',\n",
       " 'different': 'different',\n",
       " 'european': 'european',\n",
       " 'languages': 'languages',\n",
       " 'kind': 'kind',\n",
       " 'keeps': 'keeps',\n",
       " 'things': 'things',\n",
       " 'perspective': 'perspective',\n",
       " 'eh': 'eh',\n",
       " 'while': 'while',\n",
       " 'easily': 'easily',\n",
       " 'french': 'french',\n",
       " 'german': 'german',\n",
       " 'italian': 'italian',\n",
       " 'spanish': 'spanish',\n",
       " 'dutch': 'dutch',\n",
       " 'etc': 'etc',\n",
       " 'room': 'room',\n",
       " 'with': 'with',\n",
       " 'chinese': 'chinese',\n",
       " 'japanese': 'japanese',\n",
       " 'korean': 'korean',\n",
       " 'vietnamese': 'vietnamese',\n",
       " 'thai': 'thai',\n",
       " 'speaker': 'speaker',\n",
       " 'probably': 'probably',\n",
       " 'if': 'if',\n",
       " 'saw': 'saw',\n",
       " 'written': 'written',\n",
       " 'id': 'id',\n",
       " 'somewhat': 'somewhat',\n",
       " 'luck': 'luck',\n",
       " 'though': 'though',\n",
       " 'has': 'has',\n",
       " 'than': 'than',\n",
       " 'think': 'think',\n",
       " 'them': 'them',\n",
       " 'apart': 'apart',\n",
       " 'maybe': 'maybe',\n",
       " 'suggested': 'suggested',\n",
       " 'robinson': 'robinson',\n",
       " 'vocabulary': 'vocabulary',\n",
       " 'certainly': 'certainly',\n",
       " 'interesting': 'interesting',\n",
       " 'parallel': 'parallel',\n",
       " 'but': 'but',\n",
       " 'belongs': 'belongs',\n",
       " 'here': 'here',\n",
       " 'they': 'they',\n",
       " 'were': 'were',\n",
       " 'both': 'both',\n",
       " 'artists': 'artists',\n",
       " 'their': 'their',\n",
       " 'own': 'own',\n",
       " 'style': 'style',\n",
       " 'known': 'known',\n",
       " 'on': 'on',\n",
       " 'sides': 'sides',\n",
       " 'pond': 'pond',\n",
       " 'although': 'although',\n",
       " 'use': 'use',\n",
       " 'descriptive': 'descriptive',\n",
       " 'adjectives': 'adjectives',\n",
       " 'split': 'split',\n",
       " 'rate': 'rate',\n",
       " 'cant': 'cant',\n",
       " 'quite': 'quite',\n",
       " 'considered': 'considered',\n",
       " 'translations': 'translations',\n",
       " 'because': 'because',\n",
       " 'adjective': 'adjective',\n",
       " 'specific': 'specific',\n",
       " 'describing': 'describing',\n",
       " 'overly': 'overly',\n",
       " 'complex': 'complex',\n",
       " 'mechanical': 'mechanical',\n",
       " 'device': 'device',\n",
       " 'series': 'series',\n",
       " 'actions': 'actions',\n",
       " 'contrast': 'contrast',\n",
       " 'american': 'american',\n",
       " 'bum': 'bum',\n",
       " 'homeless': 'homeless',\n",
       " 'person': 'person',\n",
       " 'butt': 'butt',\n",
       " 'flat': 'flat',\n",
       " 'apartment': 'apartment',\n",
       " 'rubbish': 'rubbish',\n",
       " 'trash': 'trash',\n",
       " 'granted': 'granted',\n",
       " 'agree': 'agree',\n",
       " 'fag': 'fag',\n",
       " 'cigarette': 'cigarette',\n",
       " 'underground': 'underground',\n",
       " 'subway': 'subway',\n",
       " 'do': 'do',\n",
       " 'some': 'some',\n",
       " 'actual': 'actual',\n",
       " 'research': 'research',\n",
       " 'come': 'come',\n",
       " 'back': 'back',\n",
       " 'fiddle': 'fiddle',\n",
       " 'list': 'list',\n",
       " 'americans': 'americans',\n",
       " 'understand': 'understand',\n",
       " 'lesser': 'lesser',\n",
       " 'degree': 'degree',\n",
       " 'those': 'those',\n",
       " 'point': 'point',\n",
       " 'will': 'will',\n",
       " 'thats': 'thats',\n",
       " 'brit': 'brit',\n",
       " 'bin': 'bin',\n",
       " 'likely': 'likely',\n",
       " 'sense': 'sense',\n",
       " 'bullshit': 'bullshit',\n",
       " 'deleted': 'deleted',\n",
       " 'following': 'following',\n",
       " 'pair': 'pair',\n",
       " 'limited': 'limited',\n",
       " 'ltd': 'ltd',\n",
       " 'incorporated': 'incorporated',\n",
       " 'since': 'since',\n",
       " 'actually': 'actually',\n",
       " 'corporation': 'corporation',\n",
       " 'liability': 'liability',\n",
       " 'also': 'also',\n",
       " 'unlimited': 'unlimited',\n",
       " 'corporations': 'corporations',\n",
       " 'no': 'no',\n",
       " 'australian': 'australian',\n",
       " 'roughly': 'roughly',\n",
       " 'equivalent': 'equivalent',\n",
       " 'llc': 'llc',\n",
       " 'was': 'was',\n",
       " 'common': 'common',\n",
       " 'pocket': 'pocket',\n",
       " 'lamp': 'lamp',\n",
       " 'which': 'which',\n",
       " 'recognised': 'recognised',\n",
       " 'call': 'call',\n",
       " 'labeled': 'labeled',\n",
       " 'manufacturers': 'manufacturers',\n",
       " 'imo': 'imo',\n",
       " 'colloquial': 'colloquial',\n",
       " 'correct': 'correct',\n",
       " 'usage': 'usage',\n",
       " 'my': 'my',\n",
       " 'dictionary': 'dictionary',\n",
       " 'said': 'said',\n",
       " 'am': 'am',\n",
       " 'oxford': 'oxford',\n",
       " 'carried': 'carried',\n",
       " 'meaning': 'meaning',\n",
       " 'photography': 'photography',\n",
       " 'ill': 'ill',\n",
       " 'remove': 'remove',\n",
       " 'again': 'again',\n",
       " 'please': 'please',\n",
       " 'relate': 'relate',\n",
       " 'hole': 'hole',\n",
       " 'increases': 'increases',\n",
       " 'cancer': 'cancer',\n",
       " 'provide': 'provide',\n",
       " 'figures': 'figures',\n",
       " 'otherwise': 'otherwise',\n",
       " 'biased': 'biased',\n",
       " 'toward': 'toward',\n",
       " 'view': 'view',\n",
       " 'instead': 'instead',\n",
       " 'being': 'being',\n",
       " 'neutral': 'neutral',\n",
       " 'ed': 'ed',\n",
       " 'poor': 'poor',\n",
       " 'interpreted': 'interpreted',\n",
       " 'language': 'language',\n",
       " 'source': 'source',\n",
       " 'read': 'read',\n",
       " 'command': 'command',\n",
       " 'software': 'software',\n",
       " 'tool': 'tool',\n",
       " 'called': 'called',\n",
       " 'using': 'using',\n",
       " 'crappy': 'crappy',\n",
       " 'technique': 'technique',\n",
       " 'nowadays': 'nowadays',\n",
       " 'perl': 'perl',\n",
       " 'compiled': 'compiled',\n",
       " 'elaborate': 'elaborate',\n",
       " 'time': 'time',\n",
       " 'script': 'script',\n",
       " 'run': 'run',\n",
       " 'runs': 'runs',\n",
       " 'least': 'least',\n",
       " 'usual': 'usual',\n",
       " '2001': '2001',\n",
       " 'dec': 'dec',\n",
       " 'compile': 'compile',\n",
       " 'tree': 'tree',\n",
       " 'machine': 'machine',\n",
       " 'believe': 'believe',\n",
       " 'java': 'java',\n",
       " '56': '56',\n",
       " 'version': 'version',\n",
       " 'possible': 'possible',\n",
       " 'produce': 'produce',\n",
       " 'program': 'program',\n",
       " 'save': 'save',\n",
       " 'compilation': 'compilation',\n",
       " 'step': 'step',\n",
       " 'converted': 'converted',\n",
       " 'full': 'full',\n",
       " 'well': 'well',\n",
       " 'im': 'im',\n",
       " 'sure': 'sure',\n",
       " 'about': 'about',\n",
       " 'produces': 'produces',\n",
       " 'compiler': 'compiler',\n",
       " 'resulting': 'resulting',\n",
       " 'consists': 'consists',\n",
       " 'enough': 'enough',\n",
       " 'engine': 'engine',\n",
       " 'tend': 'tend',\n",
       " 'rather': 'rather',\n",
       " 'large': 'large',\n",
       " 'advantage': 'advantage',\n",
       " 'able': 'able',\n",
       " 'machines': 'machines',\n",
       " 'necessarily': 'necessarily',\n",
       " 'installed': 'installed',\n",
       " 'versions': 'versions',\n",
       " 'windows': 'windows',\n",
       " 'platforms': 'platforms',\n",
       " 'wesley': 'wesley',\n",
       " 'old': 'old',\n",
       " 'type': 'type',\n",
       " 'programming': 'programming',\n",
       " 'into': 'into',\n",
       " 'translated': 'translated',\n",
       " 'understandable': 'understandable',\n",
       " 'often': 'often',\n",
       " 'mistake': 'mistake',\n",
       " 'refer': 'refer',\n",
       " 'either': 'either',\n",
       " 'implemented': 'implemented',\n",
       " 'behave': 'behave',\n",
       " 'partially': 'partially',\n",
       " 'form': 'form',\n",
       " 'too': 'too',\n",
       " 'might': 'might',\n",
       " 'specifically': 'specifically',\n",
       " 'designed': 'designed',\n",
       " 'favor': 'favor',\n",
       " 'example': 'example',\n",
       " 'what': 'what',\n",
       " 'used': 'used',\n",
       " 'virtual': 'virtual',\n",
       " 'achieve': 'achieve',\n",
       " 'many': 'many',\n",
       " 'still': 'still',\n",
       " 'these': 'these',\n",
       " 'programs': 'programs',\n",
       " 'slowly': 'slowly',\n",
       " 'created': 'created',\n",
       " 'typical': 'typical',\n",
       " 'c': 'c',\n",
       " 'later': 'later',\n",
       " 'suffered': 'suffered',\n",
       " 'reputation': 'reputation',\n",
       " 'producing': 'producing',\n",
       " 'slow': 'slow',\n",
       " 'recent': 'recent',\n",
       " 'faster': 'faster',\n",
       " 'multiple': 'multiple',\n",
       " 'always': 'always',\n",
       " 'could': 'could',\n",
       " 'worded': 'worded',\n",
       " 'better': 'better',\n",
       " 'straight': 'straight',\n",
       " 'down': 'down',\n",
       " 'above': 'above',\n",
       " 'claim': 'claim',\n",
       " 'differentiate': 'differentiate',\n",
       " 'imho': 'imho',\n",
       " 'incorrect': 'incorrect',\n",
       " 'every': 'every',\n",
       " 'lets': 'lets',\n",
       " 'construct': 'construct',\n",
       " 'evaluate': 'evaluate',\n",
       " 'statements': 'statements',\n",
       " 'fly': 'fly',\n",
       " 'never': 'never',\n",
       " 'mess': 'mess',\n",
       " 'been': 'been',\n",
       " 'caused': 'caused',\n",
       " 'lack': 'lack',\n",
       " 'clear': 'clear',\n",
       " 'definition': 'definition',\n",
       " 'two': 'two',\n",
       " 'distinct': 'distinct',\n",
       " 'meanings': 'meanings',\n",
       " 'assembly': 'assembly',\n",
       " 'other': 'other',\n",
       " 'internal': 'internal',\n",
       " 'wrong': 'wrong',\n",
       " 'second': 'second',\n",
       " 'case': 'case',\n",
       " 'virtually': 'virtually',\n",
       " 'uses': 'uses',\n",
       " 'all': 'all',\n",
       " 'converts': 'converts',\n",
       " 'another': 'another',\n",
       " 'object': 'object',\n",
       " 'output': 'output',\n",
       " 'separate': 'separate',\n",
       " 'fixed': 'fixed',\n",
       " 'link': 'link',\n",
       " 'removed': 'removed',\n",
       " 'homeopathy': 'homeopathy',\n",
       " 'legitimate': 'legitimate',\n",
       " 'even': 'even',\n",
       " 'total': 'total',\n",
       " 'pseudoscientific': 'pseudoscientific',\n",
       " 'nonsense': 'nonsense',\n",
       " 'taken': 'taken',\n",
       " 'seriously': 'seriously',\n",
       " 'willing': 'willing',\n",
       " 'tolerate': 'tolerate',\n",
       " 'sympathetic': 'sympathetic',\n",
       " 'historical': 'historical',\n",
       " 'treatment': 'treatment',\n",
       " 'pages': 'pages',\n",
       " 'real': 'real',\n",
       " 'science': 'science',\n",
       " 'shouldnt': 'shouldnt',\n",
       " 'indisputable': 'indisputable',\n",
       " 'why': 'why',\n",
       " 'dispute': 'dispute',\n",
       " 'note': 'note',\n",
       " 'same': 'same',\n",
       " 'advocates': 'advocates',\n",
       " 'connection': 'connection',\n",
       " 'trustworthy': 'trustworthy',\n",
       " 'when': 'when',\n",
       " 'supports': 'supports',\n",
       " 'position': 'position',\n",
       " 'comment': 'comment',\n",
       " 'mine': 'mine',\n",
       " 'agenda': 'agenda',\n",
       " 'scientifically': 'scientifically',\n",
       " 'informed': 'informed',\n",
       " 'creative': 'creative',\n",
       " 'definitions': 'definitions',\n",
       " 'insurance': 'insurance',\n",
       " 'properly': 'properly',\n",
       " 'applied': 'applied',\n",
       " 'destruction': 'destruction',\n",
       " 'fine': 'fine',\n",
       " 'criticism': 'criticism',\n",
       " 'write': 'write',\n",
       " 'up': 'up',\n",
       " 'man': 'man',\n",
       " 'cell': 'cell',\n",
       " 'hunter': 'hunter',\n",
       " 'easy': 'easy',\n",
       " 'differ': 'differ',\n",
       " 'sentence': 'sentence',\n",
       " 'quote': 'quote',\n",
       " 'absolutely': 'absolutely',\n",
       " 'arent': 'arent',\n",
       " 'familiar': 'familiar',\n",
       " 'underlying': 'underlying',\n",
       " 'theory': 'theory',\n",
       " 'eg': 'eg',\n",
       " 'employed': 'employed',\n",
       " 'nuclear': 'nuclear',\n",
       " 'warfare': 'warfare',\n",
       " 'guiding': 'guiding',\n",
       " 'nor': 'nor',\n",
       " 'structure': 'structure',\n",
       " 'kept': 'kept',\n",
       " 'ira': 'ira',\n",
       " 'broken': 'broken',\n",
       " 'fault': 'fault',\n",
       " 'fix': 'fix',\n",
       " 'explain': 'explain',\n",
       " 'theres': 'theres',\n",
       " 'nothing': 'nothing',\n",
       " 'personal': 'personal',\n",
       " 'tired': 'tired',\n",
       " 'arguing': 'arguing',\n",
       " 're': 're',\n",
       " 'turns': 'turns',\n",
       " 'plenty': 'plenty',\n",
       " 'mutually': 'mutually',\n",
       " 'mutual': 'mutual',\n",
       " 'apply': 'apply',\n",
       " 'moving': 'moving',\n",
       " 'assured': 'assured',\n",
       " 'talk': 'talk',\n",
       " 'appealing': 'appealing',\n",
       " 'reagan': 'reagan',\n",
       " 'voters': 'voters',\n",
       " 'biases': 'biases',\n",
       " 'effectiveness': 'effectiveness',\n",
       " 'dropping': 'dropping',\n",
       " 'double': 'double',\n",
       " 'edits': 'edits',\n",
       " 'comes': 'comes',\n",
       " 'us': 'us',\n",
       " 'history': 'history',\n",
       " 'book': 'book',\n",
       " 'peace': 'peace',\n",
       " 'movement': 'movement',\n",
       " 'mad': 'mad',\n",
       " 'defined': 'defined',\n",
       " '1950': '1950',\n",
       " 'totally': 'totally',\n",
       " 'useless': 'useless',\n",
       " '2002': '2002',\n",
       " 'interest': 'interest',\n",
       " 'implication': 'implication',\n",
       " 'chosen': 'chosen',\n",
       " 'consider': 'consider',\n",
       " 'somehow': 'somehow',\n",
       " 'nonneutral': 'nonneutral',\n",
       " 'gandhi': 'gandhi',\n",
       " 'thinks': 'thinks',\n",
       " 'eye': 'eye',\n",
       " 'describes': 'describes',\n",
       " 'riots': 'riots',\n",
       " 'death': 'death',\n",
       " 'penalty': 'penalty',\n",
       " 'war': 'war',\n",
       " 'know': 'know',\n",
       " 'reality': 'reality',\n",
       " 'current': 'current',\n",
       " 'slightly': 'slightly',\n",
       " 'controversial': 'controversial',\n",
       " 'neutrality': 'neutrality',\n",
       " 'requires': 'requires',\n",
       " 'negotiation': 'negotiation',\n",
       " 'willingness': 'willingness',\n",
       " 'problem': 'problem',\n",
       " 'dislike': 'dislike',\n",
       " 'writing': 'writing',\n",
       " 'disregarding': 'disregarding',\n",
       " 'fundamental': 'fundamental',\n",
       " 'names': 'names',\n",
       " 'phrases': 'phrases',\n",
       " 'failing': 'failing',\n",
       " 'critical': 'critical',\n",
       " 'distinctions': 'distinctions',\n",
       " 'versus': 'versus',\n",
       " 'assurance': 'assurance',\n",
       " 'made': 'made',\n",
       " 'one': 'one',\n",
       " 'air': 'air',\n",
       " 'force': 'force',\n",
       " 'general': 'general',\n",
       " 'disservice': 'disservice',\n",
       " 'someone': 'someone',\n",
       " 'topic': 'topic',\n",
       " 'want': 'want',\n",
       " 'context': 'context',\n",
       " 'beyond': 'beyond',\n",
       " 'wasnt': 'wasnt',\n",
       " 'claimed': 'claimed',\n",
       " 'concept': 'concept',\n",
       " 'meme': 'meme',\n",
       " 'mainstream': 'mainstream',\n",
       " 'academic': 'academic',\n",
       " 'merits': 'merits',\n",
       " 'mention': 'mention',\n",
       " 'already': 'already',\n",
       " 'thin': 'thin',\n",
       " 'ground': 'ground',\n",
       " 'roll': 'roll',\n",
       " 'isnt': 'isnt',\n",
       " 'phrase': 'phrase',\n",
       " 'news': 'news',\n",
       " 'recently': 'recently',\n",
       " 'title': 'title',\n",
       " 'song': 'song',\n",
       " 'deserve': 'deserve',\n",
       " 'covered': 'covered',\n",
       " 'move': 'move',\n",
       " 'less': 'less',\n",
       " 'thing': 'thing',\n",
       " 'worthy': 'worthy',\n",
       " 'inclusion': 'inclusion',\n",
       " 'idiom': 'idiom',\n",
       " 'found': 'found',\n",
       " 'wouldnt': 'wouldnt',\n",
       " 'find': 'find',\n",
       " 'new': 'new',\n",
       " 'york': 'york',\n",
       " 'times': 'times',\n",
       " 'without': 'without',\n",
       " 'reference': 'reference',\n",
       " 'anyone': 'anyone',\n",
       " 'justification': 'justification',\n",
       " 'spelling': 'spelling',\n",
       " 'middle': 'middle',\n",
       " 'earth': 'earth',\n",
       " 'throughout': 'throughout',\n",
       " 'where': 'where',\n",
       " 'everywhere': 'everywhere',\n",
       " 'ive': 'ive',\n",
       " 'looked': 'looked',\n",
       " 'he': 'he',\n",
       " 'spells': 'spells',\n",
       " 'lowercase': 'lowercase',\n",
       " 'e': 'e',\n",
       " 'friday': 'friday',\n",
       " 'april': 'april',\n",
       " '12': '12',\n",
       " 'provided': 'provided',\n",
       " 'moved': 'moved',\n",
       " 'main': 'main',\n",
       " 'need': 'need',\n",
       " 'almost': 'almost',\n",
       " 'sunday': 'sunday',\n",
       " '14': '14',\n",
       " 'quick': 'quick',\n",
       " 'notes': 'notes',\n",
       " 'background': 'background',\n",
       " 'fundraising': 'fundraising',\n",
       " 'vs': 'vs',\n",
       " 'inheritance': 'inheritance',\n",
       " 'bonds': 'bonds',\n",
       " 'gains': 'gains',\n",
       " 'property': 'property',\n",
       " 'tax': 'tax',\n",
       " 'includes': 'includes',\n",
       " 'taxes': 'taxes',\n",
       " 'farm': 'farm',\n",
       " 'zero': 'zero',\n",
       " 'humor': 'humor',\n",
       " 'microsoft': 'microsoft',\n",
       " 'thanks': 'thanks',\n",
       " 'productive': 'productive',\n",
       " 'people': 'people',\n",
       " 'relation': 'relation',\n",
       " 'progressive': 'progressive',\n",
       " 'taxation': 'taxation',\n",
       " 'assume': 'assume',\n",
       " 'get': 'get',\n",
       " 'paid': 'paid',\n",
       " 'hold': 'hold',\n",
       " 'true': 'true',\n",
       " 'production': 'production',\n",
       " 'contribution': 'contribution',\n",
       " 'creation': 'creation',\n",
       " 'wealth': 'wealth',\n",
       " 'addition': 'addition',\n",
       " 'distort': 'distort',\n",
       " 'punish': 'punish',\n",
       " 'seem': 'seem',\n",
       " 'emotive': 'emotive',\n",
       " 'argued': 'argued',\n",
       " 'paying': 'paying',\n",
       " 'higher': 'higher',\n",
       " 'duty': 'duty',\n",
       " 'contribute': 'contribute',\n",
       " 'economy': 'economy',\n",
       " 'nation': 'nation',\n",
       " 'discuss': 'discuss',\n",
       " 'purposes': 'purposes',\n",
       " 'include': 'include',\n",
       " 'income': 'income',\n",
       " 'rich': 'rich',\n",
       " 'state': 'state',\n",
       " 'money': 'money',\n",
       " 'exact': 'exact',\n",
       " 'opposite': 'opposite',\n",
       " 'purpose': 'purpose',\n",
       " 'expense': 'expense',\n",
       " 'necessary': 'necessary',\n",
       " 'idea': 'idea',\n",
       " 'behind': 'behind',\n",
       " 'parent': 'parent',\n",
       " 'confrontation': 'confrontation',\n",
       " 'rare': 'rare',\n",
       " 'children': 'children',\n",
       " 'cool': 'cool',\n",
       " 'adult': 'adult',\n",
       " 'child': 'child',\n",
       " 'attention': 'attention',\n",
       " 'course': 'course',\n",
       " 'our': 'our',\n",
       " 'electronic': 'electronic',\n",
       " 'society': 'society',\n",
       " 'hardly': 'hardly',\n",
       " 'ever': 'ever',\n",
       " 'parents': 'parents',\n",
       " 'anyways': 'anyways',\n",
       " 'replacement': 'replacement',\n",
       " 'spending': 'spending',\n",
       " 'ones': 'ones',\n",
       " 'most': 'most',\n",
       " 'invented': 'invented',\n",
       " 'punishment': 'punishment',\n",
       " 'acceptable': 'acceptable',\n",
       " 'bad': 'bad',\n",
       " 'theyre': 'theyre',\n",
       " 'physical': 'physical',\n",
       " 'abuse': 'abuse',\n",
       " 'ark': 'ark',\n",
       " 'done': 'done',\n",
       " 'hum': 'hum',\n",
       " 'brought': 'brought',\n",
       " 'naming': 'naming',\n",
       " 'conflict': 'conflict',\n",
       " 'cities': 'cities',\n",
       " 'named': 'named',\n",
       " 'paris': 'paris',\n",
       " 'over': 'over',\n",
       " 'places': 'places',\n",
       " 'conflicts': 'conflicts',\n",
       " 'canada': 'canada',\n",
       " 'ontario': 'ontario',\n",
       " 'rest': 'rest',\n",
       " 'city': 'city',\n",
       " 'convention': 'convention',\n",
       " 'worked': 'worked',\n",
       " 'mailing': 'mailing',\n",
       " 'several': 'several',\n",
       " '–': '–',\n",
       " 'nice': 'nice',\n",
       " 'systemic': 'systemic',\n",
       " 'few': 'few',\n",
       " 'exceptions': 'exceptions',\n",
       " 'disambiguate': 'disambiguate',\n",
       " 'each': 'each',\n",
       " 'canadian': 'canadian',\n",
       " 'become': 'become',\n",
       " 'articles': 'articles',\n",
       " 'asked': 'asked',\n",
       " 'whether': 'whether',\n",
       " 'policy': 'policy',\n",
       " 'wikipedianeutral': 'wikipedianeutral',\n",
       " 'discusses': 'discusses',\n",
       " 'length': 'length',\n",
       " 'subject': 'subject',\n",
       " 'whilst': 'whilst',\n",
       " 'youre': 'youre',\n",
       " 'defenders': 'defenders',\n",
       " 'clearer': 'clearer',\n",
       " 'abortion': 'abortion',\n",
       " 'edited': 'edited',\n",
       " 'continue': 'continue',\n",
       " 'question': 'question',\n",
       " 'sometimes': 'sometimes',\n",
       " 'youll': 'youll',\n",
       " 'learn': 'learn',\n",
       " 'working': 'working',\n",
       " 'open': 'open',\n",
       " 'awesome': 'awesome',\n",
       " 'living': 'living',\n",
       " 'growth': 'growth',\n",
       " 'information': 'information',\n",
       " 'gentle': 'gentle',\n",
       " 'newbie': 'newbie',\n",
       " 'btw': 'btw',\n",
       " 'server': 'server',\n",
       " 'serving': 'serving',\n",
       " 'plans': 'plans',\n",
       " 'upgrade': 'upgrade',\n",
       " 'host': 'host',\n",
       " 'classification': 'classification',\n",
       " 'table': 'table',\n",
       " 'angry': 'angry',\n",
       " 'npov': 'npov',\n",
       " 'let': 'let',\n",
       " 'care': 'care',\n",
       " 'deeply': 'deeply',\n",
       " 'week': 'week',\n",
       " 'until': 'until',\n",
       " 'calm': 'calm',\n",
       " 'conform': 'conform',\n",
       " 'important': 'important',\n",
       " 'anger': 'anger',\n",
       " 'require': 'require',\n",
       " 'describe': 'describe',\n",
       " 'hats': 'hats',\n",
       " 'off': 'off',\n",
       " 'wheres': 'wheres',\n",
       " 'larry': 'larry',\n",
       " 'him': 'him',\n",
       " 'fifth': 'fifth',\n",
       " 'tuning': 'tuning',\n",
       " 'perfect': 'perfect',\n",
       " 'exist': 'exist',\n",
       " 'octave': 'octave',\n",
       " 'twice': 'twice',\n",
       " 'frequency': 'frequency',\n",
       " 'according': 'according',\n",
       " 'piano': 'piano',\n",
       " 'itself': 'itself',\n",
       " 'sharp': 'sharp',\n",
       " 'tried': 'tried',\n",
       " 'besides': 'besides',\n",
       " 'strings': 'strings',\n",
       " 'compared': 'compared',\n",
       " 'original': 'original',\n",
       " 'condensed': 'condensed',\n",
       " 'promised': 'promised',\n",
       " 'fair': 'fair',\n",
       " 'rewritten': 'rewritten',\n",
       " 'completely': 'completely',\n",
       " 'however': 'however',\n",
       " 'copyright': 'copyright',\n",
       " 'violation': 'violation',\n",
       " 'included': 'included',\n",
       " 'site': 'site',\n",
       " 'below': 'below',\n",
       " 'questions': 'questions',\n",
       " 'answered': 'answered',\n",
       " 'prove': 'prove',\n",
       " 'copernicus': 'copernicus',\n",
       " 'prussian': 'prussian',\n",
       " 'deserves': 'deserves',\n",
       " 'couple': 'couple',\n",
       " 'lines': 'lines',\n",
       " 'bearing': 'bearing',\n",
       " 'helping': 'helping',\n",
       " 'help': 'help',\n",
       " 'answering': 'answering',\n",
       " 'doing': 'doing',\n",
       " 'bit': 'bit',\n",
       " 'together': 'together',\n",
       " 'wellwritten': 'wellwritten',\n",
       " 'administrator': 'administrator',\n",
       " 'coin': 'coin',\n",
       " 'today': 'today',\n",
       " 'world': 'world',\n",
       " 'famous': 'famous',\n",
       " 'home': 'home',\n",
       " 'wore': 'wore',\n",
       " 'amongst': 'amongst',\n",
       " 'positions': 'positions',\n",
       " 'held': 'held',\n",
       " 'mathematician': 'mathematician',\n",
       " 'hired': 'hired',\n",
       " 'readers': 'readers',\n",
       " 'how': 'how',\n",
       " 'whole': 'whole',\n",
       " 'works': 'works',\n",
       " 'unhelpful': 'unhelpful',\n",
       " 'reader': 'reader',\n",
       " 'region': 'region',\n",
       " 'december': 'december',\n",
       " 'states': 'states',\n",
       " 'partial': 'partial',\n",
       " 'manuscript': 'manuscript',\n",
       " 'appears': 'appears',\n",
       " 'random': 'random',\n",
       " 'doesnt': 'doesnt',\n",
       " 'church': 'church',\n",
       " 'canon': 'canon',\n",
       " 'physician': 'physician',\n",
       " 'his': 'his',\n",
       " 'reform': 'reform',\n",
       " 'practical': 'practical',\n",
       " 'situation': 'situation',\n",
       " 'prussia': 'prussia',\n",
       " 'parts': 'parts',\n",
       " 'europe': 'europe',\n",
       " 'years': 'years',\n",
       " 'before': 'before',\n",
       " 'during': 'during',\n",
       " 'after': 'after',\n",
       " 'reformation': 'reformation',\n",
       " 'became': 'became',\n",
       " 'insecure': 'insecure',\n",
       " 'confused': 'confused',\n",
       " 'died': 'died',\n",
       " 'fits': 'fits',\n",
       " 'fought': 'fought',\n",
       " 'keeping': 'keeping',\n",
       " 'government': 'government',\n",
       " 'catholic': 'catholic',\n",
       " 'knights': 'knights',\n",
       " 'papal': 'papal',\n",
       " 'domination': 'domination',\n",
       " 'protestant': 'protestant',\n",
       " 'crown': 'crown',\n",
       " 'poland': 'poland',\n",
       " 'lithuanian': 'lithuanian',\n",
       " 'married': 'married',\n",
       " 'imperial': 'imperial',\n",
       " 'influence': 'influence',\n",
       " 'politics': 'politics',\n",
       " 'wanted': 'wanted',\n",
       " 'give': 'give',\n",
       " 'coins': 'coins',\n",
       " 'leading': 'leading',\n",
       " 'declared': 'declared',\n",
       " 'individual': 'individual',\n",
       " 'rights': 'rights',\n",
       " 'given': 'given',\n",
       " 'emperor': 'emperor',\n",
       " 'giving': 'giving',\n",
       " 'loss': 'loss',\n",
       " 'sovereignty': 'sovereignty',\n",
       " 'helps': 'helps',\n",
       " 'former': 'former',\n",
       " 'sets': 'sets',\n",
       " 'ruled': 'ruled',\n",
       " 'treaty': 'treaty',\n",
       " 'grand': 'grand',\n",
       " 'master': 'master',\n",
       " 'info': 'info',\n",
       " 'looks': 'looks',\n",
       " 'jurisdiction': 'jurisdiction',\n",
       " 'heirs': 'heirs',\n",
       " 'began': 'began',\n",
       " 'masters': 'masters',\n",
       " 'appointment': 'appointment',\n",
       " 'fit': 'fit',\n",
       " 'involvement': 'involvement',\n",
       " 'distant': 'distant',\n",
       " 'implies': 'implies',\n",
       " 'residents': 'residents',\n",
       " 'area': 'area',\n",
       " 'appealed': 'appealed',\n",
       " 'polish': 'polish',\n",
       " 'rulers': 'rulers',\n",
       " 'objections': 'objections',\n",
       " 'mentioned': 'mentioned',\n",
       " 'members': 'members',\n",
       " 'group': 'group',\n",
       " 'notoriously': 'notoriously',\n",
       " 'jealous': 'jealous',\n",
       " 'independence': 'independence',\n",
       " 'privileges': 'privileges',\n",
       " 'answer': 'answer',\n",
       " 'viz': 'viz',\n",
       " 'infringement': 'infringement',\n",
       " 'law': 'law',\n",
       " 'cry': 'cry',\n",
       " 'regularly': 'regularly',\n",
       " 'played': 'played',\n",
       " 'emperors': 'emperors',\n",
       " 'grant': 'grant',\n",
       " 'local': 'local',\n",
       " 'rule': 'rule',\n",
       " 'hard': 'hard',\n",
       " 'keep': 'keep',\n",
       " 'having': 'having',\n",
       " 'control': 'control',\n",
       " 'economic': 'economic',\n",
       " 'sway': 'sway',\n",
       " 'th': 'th',\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mapped_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k in my_mapped_dictionary:\n",
    "    if k!=my_mapped_dictionary[k]:\n",
    "        i = i+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0040333377395006115"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1190.0/len(my_mapped_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now parse all the comments. one comment at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "def find_mapped_word(myword, badlist):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    #Check if toxic word or found in dictionary\n",
    "    if (myword in badlist) or (d.check(myword)):\n",
    "        return myword\n",
    "    #else call \n",
    "    corr_word = find_mapping_for_difficult_word(myword, badlist)\n",
    "    return  corr_word\n",
    "\n",
    "def curate_a_list_of_comments(list_of_comments, my_mapped_dictionary, badlist, new_mapping_dictionary):\n",
    "    d = enchant.Dict(\"en_US\")   \n",
    "    print ('total number of comments  = ', len(list_of_comments))\n",
    "    i = 0 \n",
    "    tic = time.time()\n",
    "    begtime = tic\n",
    "    out_list_of_comments = []\n",
    "    \n",
    "    for mycomment in list_of_comments:\n",
    "        #print (mycomment)\n",
    "        if i% 30000 == 0:\n",
    "            toc = time.time()\n",
    "            print ('Number of commnets done = ', i, ',   time = ', toc-tic)\n",
    "            tic = toc\n",
    "        i += 1\n",
    "        #Split into words and make it lower case\n",
    "        mycomment_words = mycomment.lower().split()\n",
    "        mycomment_words = [x.strip() if (len(x) < 30) else x[:30] for x in mycomment_words ]\n",
    "        #Loop through each words\n",
    "        converted_comment = []\n",
    "        for myword in mycomment_words:\n",
    "            try: \n",
    "                #Get corresponding word from mapping dictionary\n",
    "                corr_word = my_mapped_dictionary[myword]\n",
    "                converted_comment.append(corr_word)\n",
    "            except:\n",
    "                new_word = find_mapped_word(myword, badlist)\n",
    "                new_mapping_dictionary[myword] = new_word\n",
    "                converted_comment.append(new_word)\n",
    "        converted_comment = ' '.join(converted_comment)\n",
    "        out_list_of_comments.append(converted_comment)\n",
    "    return out_list_of_comments, new_mapping_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_comments = ['i am leaving', 'fuck u ass*hole', '']\n",
    "# new_mapping_dictionary = {}\n",
    "# out_list_of_comments, new_mapping_dictionary = curate_a_list_of_comments(list_of_comments, my_mapped_dictionary, badlist, new_mapping_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of comments  =  159686\n",
      "Number of commnets done =  0 ,   time =  1.0013580322265625e-05\n",
      "Number of commnets done =  30000 ,   time =  0.7951180934906006\n",
      "Number of commnets done =  60000 ,   time =  0.7791118621826172\n",
      "Number of commnets done =  90000 ,   time =  0.914949893951416\n",
      "Number of commnets done =  120000 ,   time =  0.7819921970367432\n",
      "Number of commnets done =  150000 ,   time =  0.7849538326263428\n"
     ]
    }
   ],
   "source": [
    "list_of_comments = df['comment'].values.tolist()\n",
    "new_mapping_dictionary = {}\n",
    "out_list_of_comments, new_mapping_dictionary = curate_a_list_of_comments(list_of_comments, my_mapped_dictionary, badlist, new_mapping_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the word-map-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate the mapping dictionary with newly found mapping dictionary.\n",
    "updated_mapped_dictionary = {**my_mapped_dictionary, **new_mapping_dictionary}\n",
    "mapped_filename =  '../data/word_map_dictionary_' + datetime.now().strftime(\"%Y%m%d_%H%M%S\" + '.pkl')\n",
    "pickle.dump(updated_mapped_dictionary, open(mapped_filename, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the dataframe with the converted comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the 'comment' column in the dataframe \n",
    "df['comment'] = out_list_of_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxic'] = df['toxicity_score'].apply(lambda x: int(x < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['toxicity', 'toxicity_score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values(['toxic'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>250290.0</td>\n",
       "      <td>ed i am tired anyway i certainly did not mean ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>608166.0</td>\n",
       "      <td>1 i dont think unreadable is that far off beam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>630630.0</td>\n",
       "      <td>on second thought why not instead provide a ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>682829.0</td>\n",
       "      <td>youd think neonazis would have other business ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>749900.0</td>\n",
       "      <td>the macedonian origin name is monastirgreek na...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>761965.0</td>\n",
       "      <td>nice work on this article folks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>790462.0</td>\n",
       "      <td>sorry you send this message to the wrong user</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>856388.0</td>\n",
       "      <td>the protocols isnt a conspiracy theory it is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1171026.0</td>\n",
       "      <td>id be interested to know why this page discuss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1245501.0</td>\n",
       "      <td>i was with you until the last paragraph which ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rev_id                                            comment  toxic\n",
       "42    250290.0  ed i am tired anyway i certainly did not mean ...      1\n",
       "86    608166.0  1 i dont think unreadable is that far off beam...      1\n",
       "94    630630.0  on second thought why not instead provide a ne...      1\n",
       "104   682829.0  youd think neonazis would have other business ...      1\n",
       "122   749900.0  the macedonian origin name is monastirgreek na...      1\n",
       "124   761965.0                    nice work on this article folks      1\n",
       "133   790462.0      sorry you send this message to the wrong user      1\n",
       "142   856388.0  the protocols isnt a conspiracy theory it is a...      1\n",
       "195  1171026.0  id be interested to know why this page discuss...      1\n",
       "207  1245501.0  i was with you until the last paragraph which ...      1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['toxic']==1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df.toxic\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143717"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15969"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/toxic_train.tsv', sep='\\t')\n",
    "X_test.to_csv('../data/toxic_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tokens = X_train.comment.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = gensim.models.phrases.Phrases(simple_tokens)\n",
    "tokenizer = gensim.models.phrases.Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = list(tokenizer[simple_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ed_boon',\n",
       " 'said',\n",
       " 'in',\n",
       " 'an_interview',\n",
       " 'that',\n",
       " 'there',\n",
       " 'would',\n",
       " 'be',\n",
       " 'at_least',\n",
       " 'characters',\n",
       " 'however',\n",
       " 'dont_think',\n",
       " 'we',\n",
       " 'can',\n",
       " 'report',\n",
       " 'this',\n",
       " 'because',\n",
       " 'that',\n",
       " 'was',\n",
       " 'an',\n",
       " 'estimate',\n",
       " 'in',\n",
       " 'early',\n",
       " 'development',\n",
       " 'like',\n",
       " 'in',\n",
       " 'march',\n",
       " 'or',\n",
       " 'something']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = gensim.corpora.dictionary.Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rev_id', 'comment', 'toxic'], dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASSES = ['toxic']\n",
    "targets = X_train[TARGET_CLASSES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ..., \n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 400)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGzMzy87FxszMsnOxMTOz7FxszMwsu/8Pf1E1/I1Yjw8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a55cad8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot([len(doc) for doc in tokenized_text], bins=100, kde=False, label='Number of tokens per comment.')\n",
    "plt.xlabel(\"Tokens in a comment\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim((0, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word2vec on comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.word2vec.Word2Vec(tokenized_text, window=5, size=100, min_count=2, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reference', 0.8486384153366089),\n",
       " ('source', 0.7971519827842712),\n",
       " ('verification', 0.7613769173622131),\n",
       " ('citations', 0.7610040307044983),\n",
       " ('references', 0.7443020343780518),\n",
       " ('ref', 0.7399541139602661),\n",
       " ('reliable_source', 0.7288672924041748),\n",
       " ('secondary_source', 0.728357195854187),\n",
       " ('footnote', 0.716575562953949),\n",
       " ('quote', 0.6909138560295105)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dumb', 0.8389883041381836),\n",
       " ('pathetic', 0.8294868469238281),\n",
       " ('crazy', 0.8025041818618774),\n",
       " ('silly', 0.8001160621643066),\n",
       " ('ugly', 0.7900490760803223),\n",
       " ('retarded', 0.7869418859481812),\n",
       " ('lazy', 0.7713940143585205),\n",
       " ('lame', 0.7606254816055298),\n",
       " ('funny', 0.75559401512146),\n",
       " ('fucking', 0.7516430616378784)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('stupid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec-based model\n",
    "\n",
    "Aggregate word embeddings per comment (~ tf-idf weighted averaging), and use that as an input feature in a neural net with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros((len(tokenized_text), word2vec.vector_size))\n",
    "for i, tokens in enumerate(tokenized_text):\n",
    "    tokens = [t for t in tokens if t in word2vec.wv.vocab]\n",
    "    if tokens:\n",
    "        features[i, :] = np.mean([word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Dense(256, activation='relu', input_shape=(word2vec.vector_size,)))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(features, targets, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential models\n",
    "\n",
    "Simply averaging embeddings across all terms in a comment loses interactions that can occur between words, and the importance of their position. Because of this, we will now experiment with position-aware models: LSTM and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: shifting indices by 1 as index 0 will be used for padding.\n",
    "docs = [[idx + 1 for idx in corpus_dict.doc2idx(doc)]  for doc in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(docs, maxlen=MAX_SEQ_LEN, truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239975"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx = max(c for d in docs for c in d)\n",
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array([np.random.normal(size=word2vec.vector_size)]+ # for the '0' padding word\n",
    "                      [word2vec.wv[corpus_dict[idx]]\n",
    "                      if corpus_dict[idx] in word2vec.wv.vocab\n",
    "                      else np.random.normal(size=word2vec.vector_size)\n",
    "                      for idx in range(max_idx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPool1D, Flatten, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_idx + 1, word2vec.vector_size, input_length=MAX_SEQ_LEN))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(52, 5, padding='same',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(128, 3, padding='same',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.02)))\n",
    "model.compile(Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 129345 samples, validate on 14372 samples\n",
      "Epoch 1/20\n",
      "129345/129345 [==============================] - 159s 1ms/step - loss: 0.7364 - acc: 0.8683 - val_loss: 0.4451 - val_acc: 0.8505\n",
      "Epoch 2/20\n",
      "129345/129345 [==============================] - 163s 1ms/step - loss: 0.2934 - acc: 0.9009 - val_loss: 0.4016 - val_acc: 0.8574\n",
      "Epoch 3/20\n",
      "129345/129345 [==============================] - 166s 1ms/step - loss: 0.2360 - acc: 0.9197 - val_loss: 0.3604 - val_acc: 0.8842\n",
      "Epoch 4/20\n",
      "129345/129345 [==============================] - 181s 1ms/step - loss: 0.1988 - acc: 0.9349 - val_loss: 0.3922 - val_acc: 0.8824\n",
      "Epoch 5/20\n",
      "129345/129345 [==============================] - 176s 1ms/step - loss: 0.1716 - acc: 0.9472 - val_loss: 0.4341 - val_acc: 0.8831\n",
      "Epoch 6/20\n",
      "129345/129345 [==============================] - 175s 1ms/step - loss: 0.1504 - acc: 0.9565 - val_loss: 0.4497 - val_acc: 0.8687\n",
      "Epoch 7/20\n",
      "129345/129345 [==============================] - 165s 1ms/step - loss: 0.1345 - acc: 0.9631 - val_loss: 0.5452 - val_acc: 0.8736\n",
      "Epoch 8/20\n",
      "129345/129345 [==============================] - 164s 1ms/step - loss: 0.1212 - acc: 0.9677 - val_loss: 0.5526 - val_acc: 0.8759\n",
      "Epoch 9/20\n",
      "129345/129345 [==============================] - 159s 1ms/step - loss: 0.1123 - acc: 0.9714 - val_loss: 0.5593 - val_acc: 0.8656\n",
      "Epoch 10/20\n",
      "129345/129345 [==============================] - 152s 1ms/step - loss: 0.1068 - acc: 0.9732 - val_loss: 0.5428 - val_acc: 0.8700\n",
      "Epoch 11/20\n",
      "129345/129345 [==============================] - 152s 1ms/step - loss: 0.0997 - acc: 0.9766 - val_loss: 0.6271 - val_acc: 0.8683\n",
      "Epoch 12/20\n",
      "129345/129345 [==============================] - 151s 1ms/step - loss: 0.0943 - acc: 0.9782 - val_loss: 0.6351 - val_acc: 0.8666\n",
      "Epoch 13/20\n",
      "129345/129345 [==============================] - 150s 1ms/step - loss: 0.0898 - acc: 0.9799 - val_loss: 0.5868 - val_acc: 0.8635\n",
      "Epoch 14/20\n",
      "129345/129345 [==============================] - 150s 1ms/step - loss: 0.0868 - acc: 0.9801 - val_loss: 0.6940 - val_acc: 0.8637\n",
      "Epoch 15/20\n",
      "129345/129345 [==============================] - 150s 1ms/step - loss: 0.0843 - acc: 0.9815 - val_loss: 0.6346 - val_acc: 0.8698\n",
      "Epoch 16/20\n",
      "129345/129345 [==============================] - 150s 1ms/step - loss: 0.0821 - acc: 0.9821 - val_loss: 0.6411 - val_acc: 0.8565\n",
      "Epoch 17/20\n",
      "129345/129345 [==============================] - 162s 1ms/step - loss: 0.0790 - acc: 0.9830 - val_loss: 0.6534 - val_acc: 0.8668\n",
      "Epoch 18/20\n",
      "129345/129345 [==============================] - 163s 1ms/step - loss: 0.0765 - acc: 0.9840 - val_loss: 0.7102 - val_acc: 0.8693\n",
      "Epoch 19/20\n",
      "129345/129345 [==============================] - 156s 1ms/step - loss: 0.0745 - acc: 0.9849 - val_loss: 0.7441 - val_acc: 0.8680\n",
      "Epoch 20/20\n",
      "129345/129345 [==============================] - 158s 1ms/step - loss: 0.0724 - acc: 0.9851 - val_loss: 0.6993 - val_acc: 0.8698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5706ae10>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, targets, batch_size=512, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('models/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_to_sequential_input(comment):\n",
    "    tokens = tokenizer[gensim.utils.simple_preprocess(comment)]\n",
    "    t_ids = [corpus_dict.token2id[t] + 1 for t in tokens if t in word2vec.wv.vocab and t in corpus_dict.token2id]\n",
    "    return keras.preprocessing.sequence.pad_sequences([t_ids], maxlen=MAX_SEQ_LEN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(comment):\n",
    "    test_input = [comment_to_sequential_input(comment).reshape(1, -1)]\n",
    "    for target_class, score in zip(TARGET_CLASSES, model.predict(test_input)[0]):\n",
    "        print(\"{}: {:.2f}%\".format(target_class, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 42.52%\n"
     ]
    }
   ],
   "source": [
    "comment = \"Why are we having all these people from shithole countries come here?\"\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 98.59%\n"
     ]
    }
   ],
   "source": [
    "comment = 'You suck, loser!'\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 7.19%\n"
     ]
    }
   ],
   "source": [
    "comment = \"Now is the time for all good persons to come to the aid of their country\"\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([comment_to_sequential_input(doc) for doc in X_test.comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = X_test.as_matrix(columns=['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86361074582002628"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      toxic       0.90      0.94      0.92     13581\n",
      "\n",
      "avg / total       0.85      0.86      0.85     15969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmohamm/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 1\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['toxic']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
