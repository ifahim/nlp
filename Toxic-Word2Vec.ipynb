{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction.\n",
    "\n",
    "These are some (modest) attempts at participating in Jigsaw's toxic comments classification problem. For now, I am not using any external data, only the training data given (which is limiting as it's a tiny dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/fmohamm/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import string\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/toxicity_annotated_comments.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  year  \\\n",
       "0   2232.0  This:NEWLINE_TOKEN:One can make an analogy in ...  2002   \n",
       "1   4216.0  `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...  2002   \n",
       "2   8953.0                          Elected or Electoral? JHK  2002   \n",
       "3  26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...  2002   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...  2002   \n",
       "\n",
       "   logged_in       ns  sample  split  \n",
       "0       True  article  random  train  \n",
       "1       True     user  random  train  \n",
       "2      False  article  random   test  \n",
       "3       True  article  random  train  \n",
       "4       True  article  random   test  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159686"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('../data/toxicity_annotations.tsv',  sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>723</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>3989</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>3341</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>1574</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>1508</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>772</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>4020</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  worker_id  toxicity  toxicity_score\n",
       "0  2232.0        723         0             0.0\n",
       "1  2232.0       4000         0             0.0\n",
       "2  2232.0       3989         0             1.0\n",
       "3  2232.0       3341         0             0.0\n",
       "4  2232.0       1574         0             1.0\n",
       "5  2232.0       1508         0             1.0\n",
       "6  2232.0        772         0             1.0\n",
       "7  2232.0        680         0             0.0\n",
       "8  2232.0        405         0             1.0\n",
       "9  2232.0       4020         1            -1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.drop_duplicates(subset='rev_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159686"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(scores, on='rev_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159686"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>723</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>2596</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1642</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35367.0</td>\n",
       "      <td>`:In an interpreted language your source code ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>1408</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37330.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI fixe...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>691</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37346.0</td>\n",
       "      <td>`If they are ``indisputable`` then why does th...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1108</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37675.0</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44377.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe co...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  year  \\\n",
       "0   2232.0  This:NEWLINE_TOKEN:One can make an analogy in ...  2002   \n",
       "1   4216.0  `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...  2002   \n",
       "2   8953.0                          Elected or Electoral? JHK  2002   \n",
       "3  26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...  2002   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...  2002   \n",
       "5  35367.0  `:In an interpreted language your source code ...  2002   \n",
       "6  37330.0  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI fixe...  2002   \n",
       "7  37346.0  `If they are ``indisputable`` then why does th...  2002   \n",
       "8  37675.0  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002   \n",
       "9  44377.0  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe co...  2002   \n",
       "\n",
       "   logged_in       ns  sample  split  worker_id  toxicity  toxicity_score  \n",
       "0       True  article  random  train        723         0             0.0  \n",
       "1       True     user  random  train        500         0             0.0  \n",
       "2      False  article  random   test       2596         0             1.0  \n",
       "3       True  article  random  train       1642         0             1.0  \n",
       "4       True  article  random   test        202         0             1.0  \n",
       "5       True  article  random    dev       1408         0             1.0  \n",
       "6       True  article  random  train        691         0             0.0  \n",
       "7       True  article  random  train       1108         0             0.0  \n",
       "8      False  article  random    dev        403         0             1.0  \n",
       "9       True  article  random  train       1927         0             2.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['year', 'logged_in', 'split', 'ns', 'sample', 'worker_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35367.0</td>\n",
       "      <td>`:In an interpreted language your source code ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37330.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI fixe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37346.0</td>\n",
       "      <td>`If they are ``indisputable`` then why does th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37675.0</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44377.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe co...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  toxicity  \\\n",
       "0   2232.0  This:NEWLINE_TOKEN:One can make an analogy in ...         0   \n",
       "1   4216.0  `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...         0   \n",
       "2   8953.0                          Elected or Electoral? JHK         0   \n",
       "3  26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...         0   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...         0   \n",
       "5  35367.0  `:In an interpreted language your source code ...         0   \n",
       "6  37330.0  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENI fixe...         0   \n",
       "7  37346.0  `If they are ``indisputable`` then why does th...         0   \n",
       "8  37675.0  `-NEWLINE_TOKENThis is not ``creative``.  Thos...         0   \n",
       "9  44377.0  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe co...         0   \n",
       "\n",
       "   toxicity_score  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             1.0  \n",
       "3             1.0  \n",
       "4             1.0  \n",
       "5             1.0  \n",
       "6             0.0  \n",
       "7             0.0  \n",
       "8             1.0  \n",
       "9             2.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def mr_clean(comment):\n",
    "    comment = re.sub('NEWLINE_TOKEN', '', comment)\n",
    "    comment = re_tok.sub('', comment)             # remove punctuation\n",
    "    comment = re.sub('_', ' ', comment)\n",
    "    comment = re.sub( '\\s+', ' ', comment)\n",
    "    comment = comment.strip()\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(mr_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ThisOne can make an analogy in mathematical te...\n",
       "1    Clarification for you and Zundarks right i sho...\n",
       "2                             Elected or Electoral JHK\n",
       "3    This is such a fun entry DevotchkaI once had a...\n",
       "4    Please relate the ozone hole to increases in c...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['سكس',\n",
       " 'طيز',\n",
       " 'شرج',\n",
       " 'لعق',\n",
       " 'لحس',\n",
       " 'مص',\n",
       " 'تمص',\n",
       " 'بيضان',\n",
       " 'ثدي',\n",
       " 'بز',\n",
       " 'بزاز',\n",
       " 'حلمة',\n",
       " 'مفلقسة',\n",
       " 'بظر',\n",
       " 'كس',\n",
       " 'فرج',\n",
       " 'شهوة',\n",
       " 'شاذ',\n",
       " 'مبادل',\n",
       " 'عاهرة',\n",
       " 'جماع',\n",
       " 'قضيب',\n",
       " 'زب',\n",
       " 'لوطي',\n",
       " 'لواط',\n",
       " 'سحاق',\n",
       " 'سحاقية',\n",
       " 'اغتصاب',\n",
       " 'خنثي',\n",
       " 'احتلام',\n",
       " 'نيك',\n",
       " 'متناك',\n",
       " 'متناكة',\n",
       " 'شرموطة',\n",
       " 'عرص',\n",
       " 'خول',\n",
       " 'قحبة',\n",
       " 'لبوةbordel',\n",
       " 'buzna',\n",
       " 'čumět',\n",
       " 'čurák',\n",
       " 'debil',\n",
       " 'do piče',\n",
       " 'do prdele',\n",
       " 'dršťka',\n",
       " 'držka',\n",
       " 'flundra',\n",
       " 'hajzl',\n",
       " 'hovno',\n",
       " 'chcanky',\n",
       " 'chuj',\n",
       " 'jebat',\n",
       " 'kokot',\n",
       " 'kokotina',\n",
       " 'koňomrd',\n",
       " 'kunda',\n",
       " 'kurva',\n",
       " 'mamrd',\n",
       " 'mrdat',\n",
       " 'mrdka',\n",
       " 'mrdník',\n",
       " 'oslošoust',\n",
       " 'piča',\n",
       " 'píčus',\n",
       " 'píchat',\n",
       " 'pizda',\n",
       " 'prcat',\n",
       " 'prdel',\n",
       " 'prdelka',\n",
       " 'sračka',\n",
       " 'srát',\n",
       " 'šoustat',\n",
       " 'šulin',\n",
       " 'vypíčenec',\n",
       " 'zkurvit',\n",
       " 'zkurvysyn',\n",
       " 'zmrd',\n",
       " 'žrát',\n",
       " 'anus',\n",
       " 'bøsserøv',\n",
       " 'cock',\n",
       " 'fisse',\n",
       " 'fissehår',\n",
       " 'fuck',\n",
       " 'hestepik',\n",
       " 'kussekryller',\n",
       " 'lort',\n",
       " 'luder',\n",
       " 'pik',\n",
       " 'pikhår',\n",
       " 'pikslugeri',\n",
       " 'piksutteri',\n",
       " 'pis',\n",
       " 'røv',\n",
       " 'røvhul',\n",
       " 'røvskæg',\n",
       " 'røvspræke',\n",
       " 'shit',\n",
       " 'analritter',\n",
       " 'arsch',\n",
       " 'arschficker',\n",
       " 'arschlecker',\n",
       " 'arschloch',\n",
       " 'bimbo',\n",
       " 'bratze',\n",
       " 'bumsen',\n",
       " 'bonze',\n",
       " 'dödel',\n",
       " 'fick',\n",
       " 'ficken',\n",
       " 'flittchen',\n",
       " 'fotze',\n",
       " 'fratze',\n",
       " 'hackfresse',\n",
       " 'hure',\n",
       " 'hurensohn',\n",
       " 'ische',\n",
       " 'kackbratze',\n",
       " 'kacke',\n",
       " 'kacken',\n",
       " 'kackwurst',\n",
       " 'kampflesbe',\n",
       " 'kanake',\n",
       " 'kimme',\n",
       " 'lümmel',\n",
       " 'milf',\n",
       " 'möpse',\n",
       " 'morgenlatte',\n",
       " 'möse',\n",
       " 'mufti',\n",
       " 'muschi',\n",
       " 'nackt',\n",
       " 'neger',\n",
       " 'nigger',\n",
       " 'nippel',\n",
       " 'nutte',\n",
       " 'onanieren',\n",
       " 'orgasmus',\n",
       " 'pimmel',\n",
       " 'pimpern',\n",
       " 'pinkeln',\n",
       " 'pissen',\n",
       " 'pisser',\n",
       " 'popel',\n",
       " 'poppen',\n",
       " 'porno',\n",
       " 'reudig',\n",
       " 'rosette',\n",
       " 'schabracke',\n",
       " 'schlampe',\n",
       " 'scheiße',\n",
       " 'scheisser',\n",
       " 'schiesser',\n",
       " 'schnackeln',\n",
       " 'schwanzlutscher',\n",
       " 'schwuchtel',\n",
       " 'tittchen',\n",
       " 'titten',\n",
       " 'vögeln',\n",
       " 'vollpfosten',\n",
       " 'wichse',\n",
       " 'wichsen',\n",
       " 'wichser',\n",
       " '2g1c',\n",
       " '2 girls 1 cup',\n",
       " 'acrotomophilia',\n",
       " 'alabama hot pocket',\n",
       " 'alaskan pipeline',\n",
       " 'anal',\n",
       " 'anilingus',\n",
       " 'apeshit',\n",
       " 'arsehole',\n",
       " 'ass',\n",
       " 'asshole',\n",
       " 'assmunch',\n",
       " 'auto erotic',\n",
       " 'autoerotic',\n",
       " 'babeland',\n",
       " 'baby batter',\n",
       " 'baby juice',\n",
       " 'ball gag',\n",
       " 'ball gravy',\n",
       " 'ball kicking',\n",
       " 'ball licking',\n",
       " 'ball sack',\n",
       " 'ball sucking',\n",
       " 'bangbros',\n",
       " 'bareback',\n",
       " 'barely legal',\n",
       " 'barenaked',\n",
       " 'bastard',\n",
       " 'bastardo',\n",
       " 'bastinado',\n",
       " 'bbw',\n",
       " 'bdsm',\n",
       " 'beaner',\n",
       " 'beaners',\n",
       " 'beaver cleaver',\n",
       " 'beaver lips',\n",
       " 'bestiality',\n",
       " 'big black',\n",
       " 'big breasts',\n",
       " 'big knockers',\n",
       " 'big tits',\n",
       " 'bimbos',\n",
       " 'birdlock',\n",
       " 'bitch',\n",
       " 'bitches',\n",
       " 'black cock',\n",
       " 'blonde action',\n",
       " 'blonde on blonde action',\n",
       " 'blowjob',\n",
       " 'blow job',\n",
       " 'blow your load',\n",
       " 'blue waffle',\n",
       " 'blumpkin',\n",
       " 'bollocks',\n",
       " 'bondage',\n",
       " 'boner',\n",
       " 'boob',\n",
       " 'boobs',\n",
       " 'booty call',\n",
       " 'brown showers',\n",
       " 'brunette action',\n",
       " 'bukkake',\n",
       " 'bulldyke',\n",
       " 'bullet vibe',\n",
       " 'bullshit',\n",
       " 'bung hole',\n",
       " 'bunghole',\n",
       " 'busty',\n",
       " 'butt',\n",
       " 'buttcheeks',\n",
       " 'butthole',\n",
       " 'camel toe',\n",
       " 'camgirl',\n",
       " 'camslut',\n",
       " 'camwhore',\n",
       " 'carpet muncher',\n",
       " 'carpetmuncher',\n",
       " 'chocolate rosebuds',\n",
       " 'circlejerk',\n",
       " 'cleveland steamer',\n",
       " 'clit',\n",
       " 'clitoris',\n",
       " 'clover clamps',\n",
       " 'clusterfuck',\n",
       " 'cocks',\n",
       " 'coprolagnia',\n",
       " 'coprophilia',\n",
       " 'cornhole',\n",
       " 'coon',\n",
       " 'coons',\n",
       " 'creampie',\n",
       " 'cum',\n",
       " 'cumming',\n",
       " 'cunnilingus',\n",
       " 'cunt',\n",
       " 'darkie',\n",
       " 'date rape',\n",
       " 'daterape',\n",
       " 'deep throat',\n",
       " 'deepthroat',\n",
       " 'dendrophilia',\n",
       " 'dick',\n",
       " 'dildo',\n",
       " 'dingleberry',\n",
       " 'dingleberries',\n",
       " 'dirty pillows',\n",
       " 'dirty sanchez',\n",
       " 'doggie style',\n",
       " 'doggiestyle',\n",
       " 'doggy style',\n",
       " 'doggystyle',\n",
       " 'dog style',\n",
       " 'dolcett',\n",
       " 'domination',\n",
       " 'dominatrix',\n",
       " 'dommes',\n",
       " 'donkey punch',\n",
       " 'double dong',\n",
       " 'double penetration',\n",
       " 'dp action',\n",
       " 'dry hump',\n",
       " 'dvda',\n",
       " 'eat my ass',\n",
       " 'ecchi',\n",
       " 'ejaculation',\n",
       " 'erotic',\n",
       " 'erotism',\n",
       " 'escort',\n",
       " 'eunuch',\n",
       " 'faggot',\n",
       " 'fecal',\n",
       " 'felch',\n",
       " 'fellatio',\n",
       " 'feltch',\n",
       " 'female squirting',\n",
       " 'femdom',\n",
       " 'figging',\n",
       " 'fingerbang',\n",
       " 'fingering',\n",
       " 'fisting',\n",
       " 'foot fetish',\n",
       " 'footjob',\n",
       " 'frotting',\n",
       " 'fuck buttons',\n",
       " 'fuckin',\n",
       " 'fucking',\n",
       " 'fucktards',\n",
       " 'fudge packer',\n",
       " 'fudgepacker',\n",
       " 'futanari',\n",
       " 'gang bang',\n",
       " 'gay sex',\n",
       " 'genitals',\n",
       " 'giant cock',\n",
       " 'girl on',\n",
       " 'girl on top',\n",
       " 'girls gone wild',\n",
       " 'goatcx',\n",
       " 'goatse',\n",
       " 'god damn',\n",
       " 'gokkun',\n",
       " 'golden shower',\n",
       " 'goodpoop',\n",
       " 'goo girl',\n",
       " 'goregasm',\n",
       " 'grope',\n",
       " 'group sex',\n",
       " 'g-spot',\n",
       " 'guro',\n",
       " 'hand job',\n",
       " 'handjob',\n",
       " 'hard core',\n",
       " 'hardcore',\n",
       " 'hentai',\n",
       " 'homoerotic',\n",
       " 'honkey',\n",
       " 'hooker',\n",
       " 'hot carl',\n",
       " 'hot chick',\n",
       " 'how to kill',\n",
       " 'how to murder',\n",
       " 'huge fat',\n",
       " 'humping',\n",
       " 'incest',\n",
       " 'intercourse',\n",
       " 'jack off',\n",
       " 'jail bait',\n",
       " 'jailbait',\n",
       " 'jelly donut',\n",
       " 'jerk off',\n",
       " 'jigaboo',\n",
       " 'jiggaboo',\n",
       " 'jiggerboo',\n",
       " 'jizz',\n",
       " 'juggs',\n",
       " 'kike',\n",
       " 'kinbaku',\n",
       " 'kinkster',\n",
       " 'kinky',\n",
       " 'knobbing',\n",
       " 'leather restraint',\n",
       " 'leather straight jacket',\n",
       " 'lemon party',\n",
       " 'lolita',\n",
       " 'lovemaking',\n",
       " 'make me come',\n",
       " 'male squirting',\n",
       " 'masturbate',\n",
       " 'menage a trois',\n",
       " 'missionary position',\n",
       " 'motherfucker',\n",
       " 'mound of venus',\n",
       " 'mr hands',\n",
       " 'muff diver',\n",
       " 'muffdiving',\n",
       " 'nambla',\n",
       " 'nawashi',\n",
       " 'negro',\n",
       " 'neonazi',\n",
       " 'nigga',\n",
       " 'nig nog',\n",
       " 'nimphomania',\n",
       " 'nipple',\n",
       " 'nipples',\n",
       " 'nsfw images',\n",
       " 'nude',\n",
       " 'nudity',\n",
       " 'nympho',\n",
       " 'nymphomania',\n",
       " 'octopussy',\n",
       " 'omorashi',\n",
       " 'one cup two girls',\n",
       " 'one guy one jar',\n",
       " 'orgasm',\n",
       " 'orgy',\n",
       " 'paedophile',\n",
       " 'paki',\n",
       " 'panties',\n",
       " 'panty',\n",
       " 'pedobear',\n",
       " 'pedophile',\n",
       " 'pegging',\n",
       " 'penis',\n",
       " 'phone sex',\n",
       " 'piece of shit',\n",
       " 'pissing',\n",
       " 'piss pig',\n",
       " 'pisspig',\n",
       " 'playboy',\n",
       " 'pleasure chest',\n",
       " 'pole smoker',\n",
       " 'ponyplay',\n",
       " 'poof',\n",
       " 'poon',\n",
       " 'poontang',\n",
       " 'punany',\n",
       " 'poop chute',\n",
       " 'poopchute',\n",
       " 'porn',\n",
       " 'pornography',\n",
       " 'prince albert piercing',\n",
       " 'pthc',\n",
       " 'pubes',\n",
       " 'pussy',\n",
       " 'queaf',\n",
       " 'queef',\n",
       " 'quim',\n",
       " 'raghead',\n",
       " 'raging boner',\n",
       " 'rape',\n",
       " 'raping',\n",
       " 'rapist',\n",
       " 'rectum',\n",
       " 'reverse cowgirl',\n",
       " 'rimjob',\n",
       " 'rimming',\n",
       " 'rosy palm',\n",
       " 'rosy palm and her 5 sisters',\n",
       " 'rusty trombone',\n",
       " 'sadism',\n",
       " 'santorum',\n",
       " 'scat',\n",
       " 'schlong',\n",
       " 'scissoring',\n",
       " 'semen',\n",
       " 'sex',\n",
       " 'sexo',\n",
       " 'sexy',\n",
       " 'shaved beaver',\n",
       " 'shaved pussy',\n",
       " 'shemale',\n",
       " 'shibari',\n",
       " 'shitblimp',\n",
       " 'shitty',\n",
       " 'shota',\n",
       " 'shrimping',\n",
       " 'skeet',\n",
       " 'slanteye',\n",
       " 'slut',\n",
       " 's&m',\n",
       " 'smut',\n",
       " 'snatch',\n",
       " 'snowballing',\n",
       " 'sodomize',\n",
       " 'sodomy',\n",
       " 'spic',\n",
       " 'splooge',\n",
       " 'splooge moose',\n",
       " 'spooge',\n",
       " 'spread legs',\n",
       " 'spunk',\n",
       " 'strap on',\n",
       " 'strapon',\n",
       " 'strappado',\n",
       " 'strip club',\n",
       " 'style doggy',\n",
       " 'suck',\n",
       " 'sucks',\n",
       " 'suicide girls',\n",
       " 'sultry women',\n",
       " 'swastika',\n",
       " 'swinger',\n",
       " 'tainted love',\n",
       " 'taste my',\n",
       " 'tea bagging',\n",
       " 'threesome',\n",
       " 'throating',\n",
       " 'tied up',\n",
       " 'tight white',\n",
       " 'tit',\n",
       " 'tits',\n",
       " 'titties',\n",
       " 'titty',\n",
       " 'tongue in a',\n",
       " 'topless',\n",
       " 'tosser',\n",
       " 'towelhead',\n",
       " 'tranny',\n",
       " 'tribadism',\n",
       " 'tub girl',\n",
       " 'tubgirl',\n",
       " 'tushy',\n",
       " 'twat',\n",
       " 'twink',\n",
       " 'twinkie',\n",
       " 'two girls one cup',\n",
       " 'undressing',\n",
       " 'upskirt',\n",
       " 'urethra play',\n",
       " 'urophilia',\n",
       " 'vagina',\n",
       " 'venus mound',\n",
       " 'vibrator',\n",
       " 'violet wand',\n",
       " 'vorarephilia',\n",
       " 'voyeur',\n",
       " 'vulva',\n",
       " 'wank',\n",
       " 'wetback',\n",
       " 'wet dream',\n",
       " 'white power',\n",
       " 'wrapping men',\n",
       " 'wrinkled starfish',\n",
       " 'xx',\n",
       " 'xxx',\n",
       " 'yaoi',\n",
       " 'yellow showers',\n",
       " 'yiffy',\n",
       " 'zoophilia',\n",
       " '🖕',\n",
       " 'bugren',\n",
       " 'bugri',\n",
       " 'bugru',\n",
       " 'ĉiesulino',\n",
       " 'ĉiesulo',\n",
       " 'diofek',\n",
       " 'diofeka',\n",
       " 'fek',\n",
       " 'feken',\n",
       " 'fekfikanto',\n",
       " 'feklekulo',\n",
       " 'fekulo',\n",
       " 'fik',\n",
       " 'fikado',\n",
       " 'fikema',\n",
       " 'fikfek',\n",
       " 'fiki',\n",
       " 'fikiĝi',\n",
       " 'fikiĝu',\n",
       " 'fikilo',\n",
       " 'fikklaŭno',\n",
       " 'fikota',\n",
       " 'fiku',\n",
       " 'forfiki',\n",
       " 'forfikiĝu',\n",
       " 'forfiku',\n",
       " 'forfurzu',\n",
       " 'forpisi',\n",
       " 'forpisu',\n",
       " 'furzulo',\n",
       " 'kacen',\n",
       " 'kaco',\n",
       " 'kacsuĉulo',\n",
       " 'kojono',\n",
       " 'piĉen',\n",
       " 'piĉo',\n",
       " 'zamenfekasesinato',\n",
       " 'asno',\n",
       " 'bollera',\n",
       " 'cabron',\n",
       " 'cabrón',\n",
       " 'caca',\n",
       " 'chupada',\n",
       " 'chupapollas',\n",
       " 'chupetón',\n",
       " 'concha',\n",
       " 'concha de tu madre',\n",
       " 'coño',\n",
       " 'coprofagía',\n",
       " 'culo',\n",
       " 'drogas',\n",
       " 'esperma',\n",
       " 'fiesta de salchichas',\n",
       " 'follador',\n",
       " 'follar',\n",
       " 'gilipichis',\n",
       " 'gilipollas',\n",
       " 'hacer una paja',\n",
       " 'haciendo el amor',\n",
       " 'heroína',\n",
       " 'hija de puta',\n",
       " 'hijaputa',\n",
       " 'hijo de puta',\n",
       " 'hijoputa',\n",
       " 'idiota',\n",
       " 'imbécil',\n",
       " 'infierno',\n",
       " 'jilipollas',\n",
       " 'kapullo',\n",
       " 'lameculos',\n",
       " 'maciza',\n",
       " 'macizorra',\n",
       " 'maldito',\n",
       " 'mamada',\n",
       " 'marica',\n",
       " 'maricón',\n",
       " 'mariconazo',\n",
       " 'martillo',\n",
       " 'mierda',\n",
       " 'nazi',\n",
       " 'orina',\n",
       " 'pedo',\n",
       " 'pervertido',\n",
       " 'pezón',\n",
       " 'pinche',\n",
       " 'prostituta',\n",
       " 'puta',\n",
       " 'racista',\n",
       " 'ramera',\n",
       " 'sádico',\n",
       " 'sexo oral',\n",
       " 'soplagaitas',\n",
       " 'soplapollas',\n",
       " 'tetas grandes',\n",
       " 'tía buena',\n",
       " 'travesti',\n",
       " 'trio',\n",
       " 'verga',\n",
       " 'vete a la mierda',\n",
       " 'آب کیر',\n",
       " 'ارگاسم',\n",
       " 'برهنه',\n",
       " 'پورن',\n",
       " 'پورنو',\n",
       " 'تجاوز',\n",
       " 'تخمی',\n",
       " 'جق',\n",
       " 'جقی',\n",
       " 'جلق',\n",
       " 'جنده',\n",
       " 'چوچول',\n",
       " 'حشر',\n",
       " 'حشری',\n",
       " 'داف',\n",
       " 'دودول',\n",
       " 'ساک زدن',\n",
       " 'سکس',\n",
       " 'سکس کردن',\n",
       " 'سکسی',\n",
       " 'سوپر',\n",
       " 'شق کردن',\n",
       " 'شهوت',\n",
       " 'شهوتی',\n",
       " 'شونبول',\n",
       " 'فیلم سوپر',\n",
       " 'کس',\n",
       " 'کس دادن',\n",
       " 'کس کردن',\n",
       " 'کسکش',\n",
       " 'کوس',\n",
       " 'کون',\n",
       " 'کون دادن',\n",
       " 'کون کردن',\n",
       " 'کونکش',\n",
       " 'کونی',\n",
       " 'کیر',\n",
       " 'کیری',\n",
       " 'لاپا',\n",
       " 'لاپایی',\n",
       " 'لاشی',\n",
       " 'لخت',\n",
       " 'لش',\n",
       " 'منی',\n",
       " 'هرزه',\n",
       " 'alfred nussi',\n",
       " 'bylsiä',\n",
       " 'haahka',\n",
       " 'haista paska',\n",
       " 'haista vittu',\n",
       " 'hatullinen',\n",
       " 'helvetisti',\n",
       " 'hevonkuusi',\n",
       " 'hevonpaska',\n",
       " 'hevonperse',\n",
       " 'hevonvittu',\n",
       " 'hevonvitunperse',\n",
       " 'hitosti',\n",
       " 'hitto',\n",
       " 'huorata',\n",
       " 'hässiä',\n",
       " 'juosten kustu',\n",
       " 'jutku',\n",
       " 'jutsku',\n",
       " 'jätkä',\n",
       " 'kananpaska',\n",
       " 'koiranpaska',\n",
       " 'kuin esterin perseestä',\n",
       " 'kulli',\n",
       " 'kullinluikaus',\n",
       " 'kuppainen',\n",
       " 'kusaista',\n",
       " 'kuseksia',\n",
       " 'kusettaa',\n",
       " 'kusi',\n",
       " 'kusipää',\n",
       " 'kusta',\n",
       " 'kyrpiintynyt',\n",
       " 'kyrpiintyä',\n",
       " 'kyrpiä',\n",
       " 'kyrpä',\n",
       " 'kyrpänaama',\n",
       " 'kyrvitys',\n",
       " 'lahtari',\n",
       " 'lutka',\n",
       " 'molo',\n",
       " 'molopää',\n",
       " 'mulkero',\n",
       " 'mulkku',\n",
       " 'mulkvisti',\n",
       " 'muna',\n",
       " 'munapää',\n",
       " 'munaton',\n",
       " 'mutakuono',\n",
       " 'mutiainen',\n",
       " 'naida',\n",
       " 'nainti',\n",
       " 'narttu',\n",
       " 'neekeri',\n",
       " 'nekru',\n",
       " 'nuolla persettä',\n",
       " 'nussia',\n",
       " 'nussija',\n",
       " 'nussinta',\n",
       " 'paljaalla',\n",
       " 'palli',\n",
       " 'pallit',\n",
       " 'paneskella',\n",
       " 'panettaa',\n",
       " 'panna',\n",
       " 'pano',\n",
       " 'pantava',\n",
       " 'paska',\n",
       " 'paskainen',\n",
       " 'paskamainen',\n",
       " 'paskanmarjat',\n",
       " 'paskantaa',\n",
       " 'paskapuhe',\n",
       " 'paskapää',\n",
       " 'paskattaa',\n",
       " 'paskiainen',\n",
       " 'paskoa',\n",
       " 'pehko',\n",
       " 'pentele',\n",
       " 'perkele',\n",
       " 'perkeleesti',\n",
       " 'persaukinen',\n",
       " 'perse',\n",
       " 'perseennuolija',\n",
       " 'perseet olalla',\n",
       " 'persereikä',\n",
       " 'perseääliö',\n",
       " 'persläpi',\n",
       " 'perspano',\n",
       " 'persvako',\n",
       " 'pilkunnussija',\n",
       " 'pillu',\n",
       " 'pillut',\n",
       " 'pipari',\n",
       " 'piru',\n",
       " 'pistää',\n",
       " 'pyllyvako',\n",
       " 'reikä',\n",
       " 'reva',\n",
       " 'ripsipiirakka',\n",
       " 'runkata',\n",
       " 'runkkari',\n",
       " 'runkkaus',\n",
       " 'runkku',\n",
       " 'ryssä',\n",
       " 'rättipää',\n",
       " 'saatanasti',\n",
       " 'suklaaosasto',\n",
       " 'tavara',\n",
       " 'toosa',\n",
       " 'tuhkaluukku',\n",
       " 'tumputtaa',\n",
       " 'turpasauna',\n",
       " 'tussu',\n",
       " 'tussukka',\n",
       " 'tussut',\n",
       " 'vakipano',\n",
       " 'vetää käteen',\n",
       " 'viiksi',\n",
       " 'vittu',\n",
       " 'vittuilla',\n",
       " 'vittuilu',\n",
       " 'vittumainen',\n",
       " 'vittuuntua',\n",
       " 'vittuuntunut',\n",
       " 'vitun',\n",
       " 'vitusti',\n",
       " 'vituttaa',\n",
       " 'vitutus',\n",
       " 'äpärä',\n",
       " 'baiser',\n",
       " 'bander',\n",
       " 'bigornette',\n",
       " 'bite',\n",
       " 'bitte',\n",
       " 'bloblos',\n",
       " 'bordel',\n",
       " 'bosser',\n",
       " 'bourré',\n",
       " 'bourrée',\n",
       " 'brackmard',\n",
       " 'branlage',\n",
       " 'branler',\n",
       " 'branlette',\n",
       " 'branleur',\n",
       " 'branleuse',\n",
       " 'brouter le cresson',\n",
       " 'cailler',\n",
       " 'chatte',\n",
       " 'chiasse',\n",
       " 'chier',\n",
       " 'chiottes',\n",
       " 'clito',\n",
       " 'con',\n",
       " 'connard',\n",
       " 'connasse',\n",
       " 'conne',\n",
       " 'couilles',\n",
       " 'cramouille',\n",
       " 'cul',\n",
       " 'déconne',\n",
       " 'déconner',\n",
       " 'drague',\n",
       " 'emmerdant',\n",
       " 'emmerder',\n",
       " 'emmerdeur',\n",
       " 'emmerdeuse',\n",
       " 'enculé',\n",
       " 'enculée',\n",
       " 'enculeur',\n",
       " 'enculeurs',\n",
       " 'enfoiré',\n",
       " 'enfoirée',\n",
       " 'étron',\n",
       " 'fille de pute',\n",
       " 'fils de pute',\n",
       " 'folle',\n",
       " 'foutre',\n",
       " 'gerbe',\n",
       " 'gerber',\n",
       " 'gouine',\n",
       " 'grande folle',\n",
       " 'grogniasse',\n",
       " 'gueule',\n",
       " 'jouir',\n",
       " 'la putain de ta mère',\n",
       " 'malpt',\n",
       " 'ménage à trois',\n",
       " 'merde',\n",
       " 'merdeuse',\n",
       " 'merdeux',\n",
       " 'meuf',\n",
       " 'nègre',\n",
       " 'nique ta mère',\n",
       " 'nique ta race',\n",
       " 'palucher',\n",
       " 'pédale',\n",
       " 'pédé',\n",
       " 'péter',\n",
       " 'pipi',\n",
       " 'pouffiasse',\n",
       " 'pousse-crotte',\n",
       " 'putain',\n",
       " 'pute',\n",
       " 'ramoner',\n",
       " 'sac à merde',\n",
       " 'salaud',\n",
       " 'salope',\n",
       " 'suce',\n",
       " 'tapette',\n",
       " 'teuf',\n",
       " 'tringler',\n",
       " 'trique',\n",
       " 'trou du cul',\n",
       " 'turlute',\n",
       " 'veuve',\n",
       " 'zigounette',\n",
       " 'zizi',\n",
       " 'noune',\n",
       " 'aand',\n",
       " 'aandu',\n",
       " 'balatkar',\n",
       " 'beti chod',\n",
       " 'bhadva',\n",
       " 'bhadve',\n",
       " 'bhandve',\n",
       " 'bhootni ke',\n",
       " 'bhosad',\n",
       " 'bhosadi ke',\n",
       " 'boobe',\n",
       " 'chakke',\n",
       " 'chinaal',\n",
       " 'chinki',\n",
       " 'chod',\n",
       " 'chodu',\n",
       " 'chodu bhagat',\n",
       " 'chooche',\n",
       " 'choochi',\n",
       " 'choot',\n",
       " 'choot ke baal',\n",
       " 'chootia',\n",
       " 'chootiya',\n",
       " 'chuche',\n",
       " 'chuchi',\n",
       " 'chudai khanaa',\n",
       " 'chudan chudai',\n",
       " 'chut',\n",
       " 'chut ke baal',\n",
       " 'chut ke dhakkan',\n",
       " 'chut maarli',\n",
       " 'chutad',\n",
       " 'chutadd',\n",
       " 'chutan',\n",
       " 'chutia',\n",
       " 'chutiya',\n",
       " 'gaand',\n",
       " 'gaandfat',\n",
       " 'gaandmasti',\n",
       " 'gaandufad',\n",
       " 'gandu',\n",
       " 'gashti',\n",
       " 'gasti',\n",
       " 'ghassa',\n",
       " 'ghasti',\n",
       " 'harami',\n",
       " 'haramzade',\n",
       " 'hawas',\n",
       " 'hawas ke pujari',\n",
       " 'hijda',\n",
       " 'hijra',\n",
       " 'jhant',\n",
       " 'jhant chaatu',\n",
       " 'jhant ke baal',\n",
       " 'jhantu',\n",
       " 'kamine',\n",
       " 'kaminey',\n",
       " 'kanjar',\n",
       " 'kutta',\n",
       " 'kutta kamina',\n",
       " 'kutte ki aulad',\n",
       " 'kutte ki jat',\n",
       " 'kuttiya',\n",
       " 'loda',\n",
       " 'lodu',\n",
       " 'lund',\n",
       " 'lund choos',\n",
       " 'lund khajoor',\n",
       " 'lundtopi',\n",
       " 'lundure',\n",
       " 'maa ki chut',\n",
       " 'maal',\n",
       " 'madar chod',\n",
       " 'mooh mein le',\n",
       " 'mutth',\n",
       " 'najayaz',\n",
       " 'najayaz aulaad',\n",
       " 'najayaz paidaish',\n",
       " 'pataka',\n",
       " 'patakha',\n",
       " 'raand',\n",
       " 'randi',\n",
       " 'saala',\n",
       " 'saala kutta',\n",
       " 'saali kutti',\n",
       " 'saali randi',\n",
       " 'suar',\n",
       " 'suar ki aulad',\n",
       " 'tatte',\n",
       " 'tatti',\n",
       " 'teri maa ka bhosada',\n",
       " 'teri maa ka boba chusu',\n",
       " 'teri maa ki chut',\n",
       " 'tharak',\n",
       " 'tharki',\n",
       " 'balfasz',\n",
       " 'balfaszok',\n",
       " 'balfaszokat',\n",
       " 'balfaszt',\n",
       " 'barmok',\n",
       " 'barmokat',\n",
       " 'barmot',\n",
       " 'barom',\n",
       " 'baszik',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a list of profane words \n",
    "badlist1 = pd.read_table('../../data_bad_words/otherwise_bad_all_lang.txt', header=None)\n",
    "badlist2 = pd.read_table('../../data_bad_words/profane1.txt', header=None, comment='#')\n",
    "badlist1 = badlist1.values.flatten().tolist()\n",
    "badlist2 = badlist2.values.flatten().tolist()\n",
    "badlist = pd.Series(badlist1 + badlist2)\n",
    "badlist = badlist.str.lower().drop_duplicates().reset_index(drop=True)\n",
    "badlist = badlist.values.tolist()\n",
    "badlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "from fuzzywuzzy import process\n",
    "from pandas import Series \n",
    "\n",
    "def containsAny(seq, aset):\n",
    "    return bool(set(aset).intersection(seq))\n",
    "    \n",
    "def preprocess_toxic_comment_in_a_line(mycomment, badlist):\n",
    "    mycomment_words = mycomment.split()\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    num_toxic_word = 0\n",
    "    curated_line = []\n",
    "    for myword in mycomment_words:\n",
    "        myword = myword[:30] #Truncate to 30 characters\n",
    "        myword_len = len(myword)\n",
    "        myword_lower = myword.lower()\n",
    "        #Check if toxic word \n",
    "        if myword_lower in badlist:\n",
    "            curated_line.append(myword)\n",
    "            num_toxic_word += 1\n",
    "            continue \n",
    "        #If myword in dictionary then simply add \n",
    "        if d.check(myword_lower): \n",
    "            curated_line.append(myword)\n",
    "            continue \n",
    "        #Get fuzzy matches to the word\n",
    "        matches = process.extract(myword_lower, badlist)\n",
    "        if myword_len == 30:\n",
    "            if matches[0][1] > 75: #Matching percent more than 75\n",
    "                curated_line.append(matches[0][0])\n",
    "                num_toxic_word += 1\n",
    "                continue\n",
    "        #If string contains any of the following characters then try to map to the closest \n",
    "        if containsAny(myword_lower, {'!', '@', '#', '$', '%', '^', '&', '*'}):\n",
    "            closest_match = matches[0]\n",
    "            for tup in matches[:5]:\n",
    "                if len(tup[0]) == len(myword_lower):\n",
    "                    closest_match = tup\n",
    "                    break\n",
    "            new_word =   closest_match[0]\n",
    "            num_toxic_word += 1\n",
    "            curated_line.append(new_word)\n",
    "            continue\n",
    "        #If everything fails. Keep as is \n",
    "        curated_line.append(myword)\n",
    "    curated_line = ' '.join(curated_line)\n",
    "    toxic_score_boost = np.true_divide(num_toxic_word, len(mycomment_words)) \n",
    "    return Series([curated_line, toxic_score_boost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mycomment = df['comment'].ix[3,:]\n",
    "# mycomment = 'Fu$#youB@stardFu$#youB@stardFu$#youB@stardFu$#youB@stardFu$#youB@stardFu$#youB@stard'\n",
    "# print(mycomment)\n",
    "# preprocess_toxic_comment_in_a_line(mycomment, badlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '–']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '‡']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '∈']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '∈']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '∈']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '–']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '—']\n"
     ]
    }
   ],
   "source": [
    "df[['comment', 'toxic_boost']] = df['comment'].apply(preprocess_toxic_comment_in_a_line, args = [badlist] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxic'] = df['toxicity_score'].apply(lambda x: int(x < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['toxicity', 'toxicity_score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values(['toxic'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['toxic']==1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df.toxic\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/toxic_train.tsv', sep='\\t')\n",
    "X_test.to_csv('../data/toxic_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tokens = X_train.comment.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = gensim.models.phrases.Phrases(simple_tokens)\n",
    "tokenizer = gensim.models.phrases.Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = list(tokenizer[simple_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ed_boon',\n",
       " 'said',\n",
       " 'in',\n",
       " 'an_interview',\n",
       " 'that',\n",
       " 'there',\n",
       " 'would',\n",
       " 'be',\n",
       " 'at_least',\n",
       " 'characters',\n",
       " 'however',\n",
       " 'dont_think',\n",
       " 'we',\n",
       " 'can',\n",
       " 'report',\n",
       " 'this',\n",
       " 'because',\n",
       " 'that',\n",
       " 'was',\n",
       " 'an',\n",
       " 'estimate',\n",
       " 'in',\n",
       " 'early',\n",
       " 'development',\n",
       " 'like',\n",
       " 'in',\n",
       " 'march',\n",
       " 'or',\n",
       " 'something']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = gensim.corpora.dictionary.Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rev_id', 'comment', 'toxic'], dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASSES = ['toxic']\n",
    "targets = X_train[TARGET_CLASSES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ..., \n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 400)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAHVNJREFUeJzt3Xu0XWV97vHvQ7jLJdzEHAINSIYC\niiFGktZLLSoEPDVg0QHaksFBowLnaLVWsD0FL3RITwXFo9QoKUHFgFwk0nAwXKoyKoEAIReQsgso\niZFYQrhJg8Bz/pjvlsV2752VZL97rb3zfMZYY8/5m+9c8zdn2PvHnOtd7yvbRERE1LRVpxOIiIjR\nL8UmIiKqS7GJiIjqUmwiIqK6FJuIiKguxSYiIqpLsYmIiOpSbCIioroUm4iIqG7rTicw3Pbcc09P\nmDCh02lERIwod9xxx3/a3mtT99/iis2ECRNYvHhxp9OIiBhRJP18c/bPY7SIiKguxSYiIqpLsYmI\niOqqFRtJ20u6TdLdklZI+kyJXyzpQUlLymtSiUvSBZJ6JC2VNLnlvWZKur+8ZrbEXy9pWdnnAkmq\ndT4REbHpanYQWA8cYfspSdsAt0i6rmz7pO0r+rQ/GphYXlOBC4GpknYHzgKmAAbukDTf9mOlzQeB\nRcACYDpwHRER0VWq3dm48VRZ3aa8BpupbQZwSdnvVmCspHHAUcBC22tLgVkITC/bdrF9q5sZ4C4B\njq11PhERsemqfmYjaYykJcAamoKxqGw6pzwqO1/SdiW2D/Bwy+4rS2yw+Mp+4hER0WWqFhvbz9ue\nBIwHDpf0GuBM4NXAG4DdgU/VzAFA0ixJiyUt/vWvf137cBER0cew9EazvQ64GZhue3V5VLYe+Gfg\n8NJsFbBvy27jS2yw+Ph+4v0df7btKban7LXXJn8BNiIiNlG1DgKS9gJ+a3udpB2AdwDnShpne3Xp\nOXYssLzsMh84XdI8mg4Cj5d21wN/L2m30u5I4EzbayU9IWkaTQeBk4CvbCivtU8/y6WLftHWObxv\n6n7tn3BERAyoZm+0ccBcSWNo7qAut32tpJtKIRKwBPhwab8AOAboAX4DnAxQisrngNtLu8/aXluW\nTwUuBnag6YWWnmgREV2oWrGxvRQ4rJ/4EQO0N3DaANvmAHP6iS8GXrN5mUZERG0ZQSAiIqpLsYmI\niOpSbCIioroUm4iIqC7FJiIiqkuxiYiI6lJsIiKiuhSbiIioLsUmIiKqS7GJiIjqUmwiIqK6FJuI\niKguxSYiIqpLsYmIiOpSbCIioroUm4iIqK7mTJ0jXrvTR0OmkI6IGEzubCIioroUm4iIqC7FJiIi\nqkuxiYiI6qoVG0nbS7pN0t2SVkj6TInvL2mRpB5Jl0natsS3K+s9ZfuElvc6s8Tvk3RUS3x6ifVI\nOqPWuURExOapeWezHjjC9uuAScB0SdOAc4HzbR8IPAacUtqfAjxW4ueXdkg6GDgBOASYDnxN0hhJ\nY4CvAkcDBwMnlrYREdFlqhUbN54qq9uUl4EjgCtKfC5wbFmeUdYp298mSSU+z/Z62w8CPcDh5dVj\n+wHbzwLzStuIiOgyVT+zKXcgS4A1wELgP4B1tp8rTVYC+5TlfYCHAcr2x4E9WuN99hkoHhERXaZq\nsbH9vO1JwHiaO5FX1zzeQCTNkrRY0uIn163tRAoREVu0YemNZnsdcDPwh8BYSb0jF4wHVpXlVcC+\nAGX7rsCjrfE++wwU7+/4s21PsT1l57G7D8k5RURE+2r2RttL0tiyvAPwDuBemqJzfGk2E7imLM8v\n65TtN9l2iZ9QeqvtD0wEbgNuByaW3m3b0nQimF/rfCIiYtPVHBttHDC39BrbCrjc9rWS7gHmSfo8\ncBdwUWl/EfAtST3AWprige0Vki4H7gGeA06z/TyApNOB64ExwBzbKyqeT0REbKJqxcb2UuCwfuIP\n0Hx+0zf+X8B7Bnivc4Bz+okvABZsdrIREVFVRhCIiIjqUmwiIqK6FJuIiKguxSYiIqpLsYmIiOpS\nbCIioroUm4iIqC7FJiIiqkuxiYiI6lJsIiKiuhSbiIioLsUmIiKqS7GJiIjqUmwiIqK6FJuIiKgu\nxSYiIqpLsYmIiOpSbCIioroUm4iIqC7FJiIiqkuxiYiI6qoVG0n7SrpZ0j2SVkj6aImfLWmVpCXl\ndUzLPmdK6pF0n6SjWuLTS6xH0hkt8f0lLSrxyyRtW+t8IiJi09W8s3kO+ITtg4FpwGmSDi7bzrc9\nqbwWAJRtJwCHANOBr0kaI2kM8FXgaOBg4MSW9zm3vNeBwGPAKRXPJyIiNlG1YmN7te07y/KTwL3A\nPoPsMgOYZ3u97QeBHuDw8uqx/YDtZ4F5wAxJAo4Arij7zwWOrXM2ERGxOYblMxtJE4DDgEUldLqk\npZLmSNqtxPYBHm7ZbWWJDRTfA1hn+7k+8f6OP0vSYkmLn1y3dgjOKCIiNkb1YiNpJ+BK4GO2nwAu\nBF4JTAJWA1+snYPt2ban2J6y89jdax8uIiL62Lrmm0vahqbQfMf2VQC2H2nZ/g3g2rK6Cti3Zffx\nJcYA8UeBsZK2Lnc3re0jIqKL1OyNJuAi4F7b57XEx7U0Ow5YXpbnAydI2k7S/sBE4DbgdmBi6Xm2\nLU0ngvm2DdwMHF/2nwlcU+t8IiJi09W8s3kj8BfAMklLSuzTNL3JJgEGHgI+BGB7haTLgXtoerKd\nZvt5AEmnA9cDY4A5tleU9/sUME/S54G7aIpbRER0mWrFxvYtgPrZtGCQfc4BzuknvqC//Ww/QNNb\nLSIiulhGEIiIiOpSbCIioroUm4iIqC7FJiIiqkuxiYiI6lJsIiKiuhSbiIioLsUmIiKqS7GJiIjq\nUmwiIqK6FJuIiKiurWIj6bW1E4mIiNGr3Tubr0m6TdKpknatmlFERIw6bRUb228G3k8zidkdki6V\n9I6qmUVExKjR9mc2tu8H/pZmDpk/Bi6Q9DNJ766VXEREjA7tfmZzqKTzgXuBI4A/tX1QWT6/Yn4R\nETEKtDt52leAbwKftv1Mb9D2LyX9bZXMIiJi1Gi32LwTeKZlmuatgO1t/8b2t6plFxERo0K7n9nc\nAOzQsr5jiUVERGxQu8Vme9tP9a6U5R3rpBQREaNNu8XmaUmTe1ckvR54ZpD2ERERv9NusfkY8D1J\nP5F0C3AZcPpgO0jaV9LNku6RtELSR0t8d0kLJd1ffu5W4pJ0gaQeSUv7FLeZpf39kma2xF8vaVnZ\n5wJJ2tgLEBER9bX7pc7bgVcDHwE+DBxk+44N7PYc8AnbBwPTgNMkHQycAdxoeyJwY1kHOBqYWF6z\ngAuhKU7AWcBU4HDgrN4CVdp8sGW/6e2cT0REDK+NGYjzDcChwGTgREknDdbY9mrbd5blJ2m+o7MP\nMAOYW5rNBY4tyzOAS9y4FRgraRxwFLDQ9lrbjwELgell2y62b7Vt4JKW94qIiC7SVtdnSd8CXgks\nAZ4v4d4/8O3sPwE4DFgE7G17ddn0K2DvsrwP8HDLbitLbLD4yn7i/R1/Fs3dEnu+ot8mERFRUbvf\ns5kCHFzuIDaKpJ2AK4GP2X6i9WMV25a00e+5sWzPBmYDHHDQodWPFxERL9XuY7TlwCs29s0lbUNT\naL5j+6oSfqQ8AqP8XFPiq2gG+uw1vsQGi4/vJx4REV2m3TubPYF7JN0GrO8N2n7XQDuUnmEXAffa\nPq9l03xgJvCF8vOalvjpkubRdAZ43PZqSdcDf9/SKeBI4EzbayU9IWkazeO5k2iG1emISxf9YqPa\nv2/qfpUyiYjoPu0Wm7M34b3fCPwFsEzSkhL7NE2RuVzSKcDPgfeWbQuAY4Ae4DfAyQClqHwOuL20\n+6zttWX5VOBimtENriuviIjoMm0VG9s/kvQHwETbN0jaERizgX1uAQb63svb+mlv4LQB3msOMKef\n+GLgNRtIPyIiOqzdKQY+CFwBfL2E9gG+XyupiIgYXdrtIHAazWOxJ+B3E6m9vFZSERExurRbbNbb\nfrZ3RdLWNN+ziYiI2KB2i82PJH0a2EHSO4DvAT+ol1ZERIwm7RabM4BfA8uAD9H0HMsMnRER0ZZ2\ne6O9AHyjvCIiIjZKu2OjPUg/n9HYPmDIM4qIiFFnY8ZG67U98B5g96FPJyIiRqN257N5tOW1yvaX\ngHdWzi0iIkaJdh+jTW5Z3YrmTqfdu6KIiNjCtVswvtiy/BzwEC+OaRYRETGodnuj/UntRCIiYvRq\n9zHaxwfb3mcKgYiIiJfYmN5ob6CZcwbgT4HbgPtrJBUREaNLu8VmPDDZ9pMAks4G/sX2n9dKLCIi\nRo92h6vZG3i2Zf3ZEouIiNigdu9sLgFuk3R1WT8WmFsnpYiIGG3a7Y12jqTrgDeX0Mm276qXVkRE\njCbtPkYD2BF4wvaXgZWS9q+UU0REjDLtTgt9FvAp4MwS2gb4dq2kIiJidGn3zuY44F3A0wC2fwns\nXCupiIgYXdotNs/aNmWaAUkv29AOkuZIWiNpeUvsbEmrJC0pr2Natp0pqUfSfZKOaolPL7EeSWe0\nxPeXtKjEL5O0bZvnEhERw6zdYnO5pK8DYyV9ELiBDU+kdjEwvZ/4+bYnldcCAEkHAycAh5R9viZp\njKQxwFeBo4GDgRNLW4Bzy3sdCDwGnNLmuURExDBrd4qBfwSuAK4EXgX8ne2vbGCfHwNr28xjBjDP\n9nrbDwI9wOHl1WP7AdvPAvOAGZIEHFFygqYb9rFtHisiIobZBrs+l7uLG8pgnAuH4JinSzoJWAx8\nwvZjwD7ArS1tVpYYwMN94lOBPYB1tp/rp31/5zALmAWw5ysGbBYREZVs8M7G9vPAC5J2HYLjXQi8\nEpgErOalUxdUY3u27Sm2p+w8NhOMRkQMt3ZHEHgKWCZpIaVHGoDt/7UxB7P9SO+ypG8A15bVVcC+\nLU3HlxgDxB+l+fxo63J309o+IiK6TLvF5qry2iySxtleXVaPA3p7qs0HLpV0HvDfgIk0o0oLmFi+\nQLqKphPB+2xb0s3A8TSf48wErtnc/CIioo5Bi42k/Wz/wvZGj4Mm6bvAW4E9Ja0EzgLeKmkSTRfq\nh4APAdheIely4B6amUBPK4/vkHQ6cD0wBphje0U5xKeAeZI+D9wFXLSxOUZExPBQ8/WZATZKd9qe\nXJavtP1nw5ZZJQccdKg/f/G1G27YRd43db9OpxARWzhJd9iesqn7b6iDgFqWD9jUg0RExJZtQ8XG\nAyxHRES0bUMdBF4n6QmaO5wdyjJl3bZ3qZpdRESMCoMWG9tjhiuRiIgYvTZmPpuIiIhNkmITERHV\npdhERER1KTYREVFdik1ERFSXYhMREdWl2ERERHUpNhERUV2KTUREVJdiExER1aXYREREdSk2ERFR\nXYpNRERUl2ITERHVpdhERER1KTYREVFdtWIjaY6kNZKWt8R2l7RQ0v3l524lLkkXSOqRtFTS5JZ9\nZpb290ua2RJ/vaRlZZ8LJKnWuURExOapeWdzMTC9T+wM4EbbE4EbyzrA0cDE8poFXAhNcQLOAqYC\nhwNn9Rao0uaDLfv1PVZERHSJasXG9o+BtX3CM4C5ZXkucGxL/BI3bgXGShoHHAUstL3W9mPAQmB6\n2baL7VttG7ik5b0iIqLLDPdnNnvbXl2WfwXsXZb3AR5uabeyxAaLr+wnHhERXahjHQTKHYmH41iS\nZklaLGnxk+v63mxFRERtw11sHimPwCg/15T4KmDflnbjS2yw+Ph+4v2yPdv2FNtTdh67+2afRERE\nbJzhLjbzgd4eZTOBa1riJ5VeadOAx8vjtuuBIyXtVjoGHAlcX7Y9IWla6YV2Ust7RUREl9m61htL\n+i7wVmBPSStpepV9Abhc0inAz4H3luYLgGOAHuA3wMkAttdK+hxwe2n3Wdu9z8FOpenxtgNwXXlF\nREQXqlZsbJ84wKa39dPWwGkDvM8cYE4/8cXAazYnx4iIGB7Vik0MnUsX/aLttu+bul/FTCIiNk2G\nq4mIiOpSbCIioroUm4iIqC7FJiIiqkuxiYiI6lJsIiKiuhSbiIioLsUmIiKqS7GJiIjqUmwiIqK6\nFJuIiKguxSYiIqrLQJyjzMYM2gkZuDMihkfubCIioroUm4iIqC7FJiIiqkuxiYiI6lJsIiKiuhSb\niIioLsUmIiKq60ixkfSQpGWSlkhaXGK7S1oo6f7yc7cSl6QLJPVIWippcsv7zCzt75c0sxPnEhER\nG9bJO5s/sT3J9pSyfgZwo+2JwI1lHeBoYGJ5zQIuhKY4AWcBU4HDgbN6C1RERHSXbnqMNgOYW5bn\nAse2xC9x41ZgrKRxwFHAQttrbT8GLASmD3fSERGxYZ0qNgZ+KOkOSbNKbG/bq8vyr4C9y/I+wMMt\n+64ssYHiERHRZTo1NtqbbK+S9HJgoaSftW60bUkeqoOVgjYLYM9XpB5FRAy3jhQb26vKzzWSrqb5\nzOURSeNsry6PydaU5quAfVt2H19iq4C39on/6wDHmw3MBjjgoEOHrIiNBhszcGcG7YyITTXsj9Ek\nvUzSzr3LwJHAcmA+0NujbCZwTVmeD5xUeqVNAx4vj9uuB46UtFvpGHBkiUVERJfpxJ3N3sDVknqP\nf6nt/yfpduBySacAPwfeW9ovAI4BeoDfACcD2F4r6XPA7aXdZ22vHb7TiIiIdg17sbH9APC6fuKP\nAm/rJ27gtAHeaw4wZ6hzjIiIodVNXZ8jImKUykyd0bZ0JoiITZU7m4iIqC7FJiIiqkuxiYiI6lJs\nIiKiuhSbiIioLsUmIiKqS9fnqGJjuklDukpHjHa5s4mIiOpyZxNdIV8YjRjdcmcTERHVpdhERER1\neYwWI04euUWMPCk2MaqlV1xEd8hjtIiIqC7FJiIiqstjtIgW+Twooo7c2URERHW5s4nYRLkLimhf\nik3EMNjYXnEbI4UsRoIRX2wkTQe+DIwBvmn7Cx1OKWJY5Q4rRoIRXWwkjQG+CrwDWAncLmm+7Xs6\nm1lEd8odVnTKiC42wOFAj+0HACTNA2YAKTYRw6xmIatlYwtk7iI33UgvNvsAD7esrwSmdiiXiBhh\nahbIkVh8axrpxaYtkmYBs8rq+vdP+4PlncynTXsC/9npJDZgJOQIyXOoJc+hNVLyfNXm7DzSi80q\nYN+W9fEl9hK2ZwOzASQttj1leNLbdCMhz5GQIyTPoZY8h9ZIynNz9h/pX+q8HZgoaX9J2wInAPM7\nnFNERPQxou9sbD8n6XTgepquz3Nsr+hwWhER0ceILjYAthcACzZil9m1chliIyHPkZAjJM+hljyH\n1haRp2wPVSIRERH9Gumf2URExAiwxRQbSdMl3SepR9IZnc6nlaSHJC2TtKS3x4ek3SUtlHR/+blb\nB/KaI2mNpOUtsX7zUuOCcn2XSprc4TzPlrSqXNMlko5p2XZmyfM+SUcNU477SrpZ0j2SVkj6aIl3\n1fUcJM9uu57bS7pN0t0lz8+U+P6SFpV8Lisdh5C0XVnvKdsndDjPiyU92HI9J5V4x36PyvHHSLpL\n0rVlfeiup+1R/6LpPPAfwAHAtsDdwMGdzqslv4eAPfvE/gE4oyyfAZzbgbzeAkwGlm8oL+AY4DpA\nwDRgUYfzPBv4q37aHlz+/bcD9i//XYwZhhzHAZPL8s7Av5dcuup6DpJnt11PATuV5W2AReU6XQ6c\nUOL/BHykLJ8K/FNZPgG4bJiu50B5Xgwc30/7jv0eleN/HLgUuLasD9n13FLubH43rI3tZ4HeYW26\n2QxgblmeCxw73AnY/jGwtk94oLxmAJe4cSswVtK4DuY5kBnAPNvrbT8I9ND891GV7dW27yzLTwL3\n0oyA0VXXc5A8B9Kp62nbT5XVbcrLwBHAFSXe93r2XucrgLdJUgfzHEjHfo8kjQfeCXyzrIshvJ5b\nSrHpb1ibwX6BhpuBH0q6Q81oBwB7215dln8F7N2Z1H7PQHl14zU+vTyKmNPyGLLjeZZHDofR/F9u\n117PPnlCl13P8shnCbAGWEhzV7XO9nP95PK7PMv2x4E9OpGn7d7reU65nudL2q5vnsVw/rt/Cfhr\n4IWyvgdDeD23lGLT7d5kezJwNHCapLe0bnRzr9p13Qa7Na/iQuCVwCRgNfDFzqbTkLQTcCXwMdtP\ntG7rpuvZT55ddz1tP297Es3IIYcDr+5wSv3qm6ek1wBn0uT7BmB34FMdTBFJ/x1YY/uOWsfYUopN\nW8PadIrtVeXnGuBqml+cR3pvn8vPNZ3L8CUGyqurrrHtR8ov+QvAN3jx0U7H8pS0Dc0f8O/YvqqE\nu+569pdnN17PXrbXATcDf0jz2Kn3+4Otufwuz7J9V+DRDuU5vTyutO31wD/T+ev5RuBdkh6i+Zjh\nCJp5wobsem4pxaZrh7WR9DJJO/cuA0cCy2nym1mazQSu6UyGv2egvOYDJ5XeNNOAx1seDw27Ps+5\nj6O5ptDkeULpTbM/MBG4bRjyEXARcK/t81o2ddX1HCjPLryee0kaW5Z3oJnT6l6aP+bHl2Z9r2fv\ndT4euKncSXYiz5+1/A+GaD4Hab2ew/7vbvtM2+NtT6D5+3iT7fczlNezdu+GbnnR9PL4d5rnun/T\n6Xxa8jqApjfP3cCK3txonn/eCNwP3ADs3oHcvkvzyOS3NM9rTxkoL5reM18t13cZMKXDeX6r5LG0\n/GKMa2n/NyXP+4CjhynHN9E8IlsKLCmvY7rteg6SZ7ddz0OBu0o+y4G/K/EDaIpdD/A9YLsS376s\n95TtB3Q4z5vK9VwOfJsXe6x17PeoJee38mJvtCG7nhlBICIiqttSHqNFREQHpdhERER1KTYREVFd\nik1ERFSXYhMREdWl2MSII2mPltFyf6WXjka8bT/tDyzDhQxHbsdJ+uRwHKtbSXq3pK78Nn90zoif\nqTO2PLYfpRk2BUlnA0/Z/seOJlXYvrrTOXSBd9OMr/WzTicS3SN3NjGqSPprScvL63/2s/3AMl/H\nZElbSzpPzXwjSyV9oLR5u6QbJV2lZo6WS1r2/z9q5npZKuncft7/A5K+VJa/LenLkv5N0gOSjhsg\n5x+UQVhX9ObQT5upkn6qZl6URZJ2lLSDpLlq5kK6s3dMvZLDVZJukPRzSR+R9Mly3v/W8o32W8r5\nLy7nNEXS1Wrm1jm75dgzyzVaIulrkrYq126dpC+UnH4q6eWS3kzzJdDzS/sJbf/jxaiWO5sYNSRN\nBd5PM7jh1sBtkv4VeKZsP4hmro6TbC+TdCrN4IOHqxl191ZJPyxvNxk4BHikxKcBD9L8IT3Etnv/\naG/Ay2nGnXotzdwg/d35zLS9VtKOwGJJV9p+rOW8tqcZr+rPbN8paVdgPfBXwHrbr5V0CLBA0sSy\n2yHlHHaiGZ3g47YPk/QV4M+B/1vaPWN7iqRPAN8HXk8zgu8DpWiOpxme5o9sPydpNs1wJpfTjIf1\nI9tnSDoP+B+2vyBpAXCF7e+3cX1iC5E7mxhN3gRcafsZN3OxfB94c9m2N80f+hNtLyuxI4GTy+c5\ni4CxNGN7Adxq+5e2n6cZsmUCzZw5LwDfKHcpT7eR0/fdWMrAQ8X/paS7gZ/S/HF/ZZ/tBwG/8Ivz\nzDxe8noTzVAn2F4B/BI4sOxzk+2nbT8CPAX8oMSXlXPpNb8lvszNgJv/RTOh33jg7TTFe3G5Tn/c\nkt8ztq8ry3f0ed+Il8idTWwp1tH8Mf4jXvwsQcCptm9sbSjp7TR3Dr2eB7a2/VtJU2gGU3wP8BGa\ngjWY1vf5vcmlyrHeAkyz/YykW2jGndpcrcd9oWX9BV76e7++nzat7QTMsf2/++S9NfBsS+h58vck\nBpE7mxhNfgIcVz7L2IlmNsGflG3ry/oHJL23xK4HTi1/OJH0KjUj8/ZLzejcu9i+FvhLmonFNteu\nwNpSaA6huYvo6x5gP5X56CXtImlMObf3l9hBNFM69wxBTq1uAN4rac9ynD0k7beBfZ6kmVI64nfy\nfyIxati+TdJ3aaaUALiwfDZzYNn+lJpJohZKehr4OrAfsETNjLZrGHy68F2Bq8rnO1vRzNe+uf4F\nmCXpHppRkxf1bWB7vaQTgQvL5zfP0Mw38hXg65KW0Yx4fZLtZzWEsx2X6/cZ4AZJW5XjfJjmLnEg\n3y15fQI41vZDQ5ZQjFgZ9TkiIqrLY7SIiKguxSYiIqpLsYmIiOpSbCIioroUm4iIqC7FJiIiqkux\niYiI6lJsIiKiuv8PkhQko2YlzTQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17187d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot([len(doc) for doc in tokenized_text], bins=100, kde=False, label='Number of tokens per comment.')\n",
    "plt.xlabel(\"Tokens in a comment\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim((0, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word2vec on comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.word2vec.Word2Vec(tokenized_text, window=5, size=100, min_count=2, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reference', 0.8324511647224426),\n",
       " ('source', 0.8086929321289062),\n",
       " ('ref', 0.7720947265625),\n",
       " ('references', 0.7694875001907349),\n",
       " ('citations', 0.7552413940429688),\n",
       " ('reliable_source', 0.7348667979240417),\n",
       " ('secondary_source', 0.7305512428283691),\n",
       " ('footnote', 0.7250741720199585),\n",
       " ('verification', 0.7163118124008179),\n",
       " ('sources', 0.7059152722358704)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dumb', 0.844036340713501),\n",
       " ('pathetic', 0.8346757292747498),\n",
       " ('crazy', 0.8233659863471985),\n",
       " ('retarded', 0.7821276187896729),\n",
       " ('funny', 0.7751038074493408),\n",
       " ('sick', 0.7745219469070435),\n",
       " ('disgusting', 0.7631621360778809),\n",
       " ('ugly', 0.761038064956665),\n",
       " ('lazy', 0.7598941326141357),\n",
       " ('silly', 0.7505162954330444)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('stupid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec-based model\n",
    "\n",
    "Aggregate word embeddings per comment (~ tf-idf weighted averaging), and use that as an input feature in a neural net with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.zeros((len(tokenized_text), word2vec.vector_size))\n",
    "for i, tokens in enumerate(tokenized_text):\n",
    "    tokens = [t for t in tokens if t in word2vec.wv.vocab]\n",
    "    if tokens:\n",
    "        features[i, :] = np.mean([word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Dense(256, activation='relu', input_shape=(word2vec.vector_size,)))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(features, targets, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential models\n",
    "\n",
    "Simply averaging embeddings across all terms in a comment loses interactions that can occur between words, and the importance of their position. Because of this, we will now experiment with position-aware models: LSTM and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: shifting indices by 1 as index 0 will be used for padding.\n",
    "docs = [[idx + 1 for idx in corpus_dict.doc2idx(doc)]  for doc in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(docs, maxlen=MAX_SEQ_LEN, truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241649"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx = max(c for d in docs for c in d)\n",
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.array([np.random.normal(size=word2vec.vector_size)]+ # for the '0' padding word\n",
    "                      [word2vec.wv[corpus_dict[idx]]\n",
    "                      if corpus_dict[idx] in word2vec.wv.vocab\n",
    "                      else np.random.normal(size=word2vec.vector_size)\n",
    "                      for idx in range(max_idx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPool1D, Flatten, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_idx + 1, word2vec.vector_size, input_length=MAX_SEQ_LEN))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(52, 5, padding='same',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(128, 3, padding='same',\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.02)))\n",
    "model.compile(Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 129345 samples, validate on 14372 samples\n",
      "Epoch 1/20\n",
      "129345/129345 [==============================] - 136s 1ms/step - loss: 0.7663 - acc: 0.8653 - val_loss: 0.4417 - val_acc: 0.8569\n",
      "Epoch 2/20\n",
      "129345/129345 [==============================] - 137s 1ms/step - loss: 0.3009 - acc: 0.8997 - val_loss: 0.3915 - val_acc: 0.8655\n",
      "Epoch 3/20\n",
      "129345/129345 [==============================] - 136s 1ms/step - loss: 0.2385 - acc: 0.9189 - val_loss: 0.3708 - val_acc: 0.8890\n",
      "Epoch 4/20\n",
      "129345/129345 [==============================] - 126s 976us/step - loss: 0.1983 - acc: 0.9358 - val_loss: 0.3755 - val_acc: 0.8902\n",
      "Epoch 5/20\n",
      "129345/129345 [==============================] - 135s 1ms/step - loss: 0.1720 - acc: 0.9463 - val_loss: 0.4404 - val_acc: 0.8844\n",
      "Epoch 6/20\n",
      "129345/129345 [==============================] - 136s 1ms/step - loss: 0.1522 - acc: 0.9552 - val_loss: 0.4782 - val_acc: 0.8651\n",
      "Epoch 7/20\n",
      "129345/129345 [==============================] - 134s 1ms/step - loss: 0.1351 - acc: 0.9625 - val_loss: 0.4832 - val_acc: 0.8796\n",
      "Epoch 8/20\n",
      "129345/129345 [==============================] - 139s 1ms/step - loss: 0.1233 - acc: 0.9673 - val_loss: 0.5115 - val_acc: 0.8736\n",
      "Epoch 9/20\n",
      "129345/129345 [==============================] - 141s 1ms/step - loss: 0.1134 - acc: 0.9710 - val_loss: 0.5303 - val_acc: 0.8746\n",
      "Epoch 10/20\n",
      "129345/129345 [==============================] - 143s 1ms/step - loss: 0.1073 - acc: 0.9732 - val_loss: 0.5443 - val_acc: 0.8720\n",
      "Epoch 11/20\n",
      "129345/129345 [==============================] - 142s 1ms/step - loss: 0.1004 - acc: 0.9756 - val_loss: 0.5832 - val_acc: 0.8724\n",
      "Epoch 12/20\n",
      "129345/129345 [==============================] - 140s 1ms/step - loss: 0.0961 - acc: 0.9775 - val_loss: 0.5855 - val_acc: 0.8553\n",
      "Epoch 13/20\n",
      "129345/129345 [==============================] - 139s 1ms/step - loss: 0.0921 - acc: 0.9787 - val_loss: 0.6493 - val_acc: 0.8689\n",
      "Epoch 14/20\n",
      "129345/129345 [==============================] - 142s 1ms/step - loss: 0.0879 - acc: 0.9803 - val_loss: 0.6509 - val_acc: 0.8723\n",
      "Epoch 15/20\n",
      "129345/129345 [==============================] - 141s 1ms/step - loss: 0.0843 - acc: 0.9816 - val_loss: 0.6943 - val_acc: 0.8777\n",
      "Epoch 16/20\n",
      "129345/129345 [==============================] - 142s 1ms/step - loss: 0.0815 - acc: 0.9824 - val_loss: 0.6305 - val_acc: 0.8639\n",
      "Epoch 17/20\n",
      "129345/129345 [==============================] - 138s 1ms/step - loss: 0.0791 - acc: 0.9829 - val_loss: 0.6559 - val_acc: 0.8720\n",
      "Epoch 18/20\n",
      "129345/129345 [==============================] - 139s 1ms/step - loss: 0.0765 - acc: 0.9843 - val_loss: 0.6627 - val_acc: 0.8708\n",
      "Epoch 19/20\n",
      "129345/129345 [==============================] - 139s 1ms/step - loss: 0.0743 - acc: 0.9845 - val_loss: 0.5983 - val_acc: 0.8736\n",
      "Epoch 20/20\n",
      "129345/129345 [==============================] - 138s 1ms/step - loss: 0.0734 - acc: 0.9848 - val_loss: 0.6792 - val_acc: 0.8624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15107fd68>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, targets, batch_size=512, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('models/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comment_to_sequential_input(comment):\n",
    "    tokens = tokenizer[gensim.utils.simple_preprocess(comment)]\n",
    "    t_ids = [corpus_dict.token2id[t] + 1 for t in tokens if t in word2vec.wv.vocab and t in corpus_dict.token2id]\n",
    "    return keras.preprocessing.sequence.pad_sequences([t_ids], maxlen=MAX_SEQ_LEN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(comment):\n",
    "    test_input = [comment_to_sequential_input(comment).reshape(1, -1)]\n",
    "    for target_class, score in zip(TARGET_CLASSES, model.predict(test_input)[0]):\n",
    "        print(\"{}: {:.2f}%\".format(target_class, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 56.00%\n"
     ]
    }
   ],
   "source": [
    "comment = \"Why are we having all these people from shithole countries come here?\"\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 99.10%\n"
     ]
    }
   ],
   "source": [
    "comment = 'You suck, loser!'\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 2.35%\n"
     ]
    }
   ],
   "source": [
    "comment = \"Now is the time for all good persons to come to the aid of their country\"\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([comment_to_sequential_input(doc) for doc in X_test.comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = X_test.as_matrix(columns=['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86655394827478238"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      toxic       0.90      0.94      0.92     13612\n",
      "\n",
      "avg / total       0.85      0.87      0.86     15969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbatz/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 1\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['toxic']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f11nd1'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell('f11nd1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fuk', 90)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Levenshtein\n",
    "# !pip install fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz, process\n",
    "# fuzz.partial_ratio('fuck', '))))0000fuck))')\n",
    "process.extract('fukfukfuk', ['fuck', 'pics', 'f4ck', 'fuk'],limit = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'big.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-b47dc728148e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'big.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'big.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
